{
  "matching_results": 38,
  "retrieval_details": {
    "document_retrieval_strategy": "untrained"
  },
  "suggested_refinements": [
    {
      "text": "in"
    },
    {
      "text": "Data"
    },
    {
      "text": "on"
    },
    {
      "text": "of"
    },
    {
      "text": "add"
    },
    {
      "text": "ibm"
    },
    {
      "text": "Cloud Pak"
    },
    {
      "text": "use"
    },
    {
      "text": "database"
    },
    {
      "text": "by"
    }
  ],
  "aggregations": [
    {
      "type": "term",
      "field": "enriched_text.entities.text",
      "name": "entities",
      "results": [
        {
          "key": "IBM",
          "matching_results": 38
        },
        {
          "key": "ibm",
          "matching_results": 32
        },
        {
          "key": "Pak",
          "matching_results": 18
        },
        {
          "key": "Watson",
          "matching_results": 16
        },
        {
          "key": "Red Hat",
          "matching_results": 15
        },
        {
          "key": "@MASTER_1_IP",
          "matching_results": 13
        },
        {
          "key": "Kubernetes",
          "matching_results": 12
        },
        {
          "key": "100 GB",
          "matching_results": 9
        },
        {
          "key": "2.1.0.1",
          "matching_results": 9
        },
        {
          "key": "MongoDB",
          "matching_results": 8
        }
      ]
    }
  ],
  "results": [
    {
      "document_id": "c86dfa48-dfd7-4899-835d-aed208972288",
      "result_metadata": {
        "collection_id": "fbafd5e1-9cf1-c24c-0000-016eeb0a5c9f",
        "document_retrieval_source": "search",
        "confidence": 0.0579
      },
      "enriched_text": [
        {
          "entities": [
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 59,
                    "begin": 52
                  },
                  "text": "Red Hat"
                },
                {
                  "location": {
                    "end": 445,
                    "begin": 438
                  },
                  "text": "Red Hat"
                }
              ],
              "text": "Red Hat",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 82,
                    "begin": 79
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 118,
                    "begin": 115
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 3827,
                    "begin": 3824
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 3863,
                    "begin": 3860
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 6339,
                    "begin": 6336
                  },
                  "text": "IBM"
                }
              ],
              "text": "IBM",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 351,
                    "begin": 345
                  },
                  "text": "Tiller"
                },
                {
                  "location": {
                    "end": 1001,
                    "begin": 995
                  },
                  "text": "Tiller"
                }
              ],
              "text": "Tiller",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 925,
                    "begin": 919
                  },
                  "text": "tiller"
                },
                {
                  "location": {
                    "end": 942,
                    "begin": 936
                  },
                  "text": "tiller"
                }
              ],
              "text": "tiller",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 1364,
                    "begin": 1359
                  },
                  "text": "64 GB"
                },
                {
                  "location": {
                    "end": 2054,
                    "begin": 2049
                  },
                  "text": "64 GB"
                }
              ],
              "text": "64 GB",
              "type": "Quantity"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 1474,
                    "begin": 1468
                  },
                  "text": "800\nGB"
                },
                {
                  "location": {
                    "end": 1573,
                    "begin": 1567
                  },
                  "text": "800\nGB"
                },
                {
                  "location": {
                    "end": 2164,
                    "begin": 2158
                  },
                  "text": "800\nGB"
                },
                {
                  "location": {
                    "end": 2263,
                    "begin": 2257
                  },
                  "text": "800\nGB"
                }
              ],
              "text": "800\nGB",
              "type": "Quantity"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 1690,
                    "begin": 1685
                  },
                  "text": "mount"
                },
                {
                  "location": {
                    "end": 2380,
                    "begin": 2375
                  },
                  "text": "mount"
                }
              ],
              "text": "mount",
              "type": "GeographicFeature"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 4189,
                    "begin": 4186
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 4250,
                    "begin": 4247
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 6459,
                    "begin": 6456
                  },
                  "text": "ibm"
                }
              ],
              "text": "ibm",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 8537,
                    "begin": 8526
                  },
                  "text": "mount point"
                }
              ],
              "text": "mount point",
              "type": "GeographicFeature"
            }
          ]
        }
      ],
      "metadata": {
        "parent_document_id": "c86dfa48-dfd7-4899-835d-aed208972288"
      },
      "extracted_metadata": {
        "sha1": "65FAD100BF485EE56518B97623FC1C87AB150ECD",
        "numPages": "8",
        "filename": "ovu-60-67.pdf",
        "file_type": "pdf"
      },
      "text": [
        "-\n-\n-\n-\n-\n-\n-\n-\n-\n Installing Cloud Pak for Data on Red Hat OpenShift\n(without IBM Cloud Private) \nYou can install IBMÂ® Cloud Pak for Data on OpenShift environment.  \n Before you begin \n \nTo install Cloud Pak for Data on OpenShift, ensure that your environment meets the\nfollowing requirements:\nSoftware requirements\nOpenShift Version 3.11\nHelm/Tiller Version 2.9.1 For more information, see Getting started with Helm\non OpenShift on the Red Hat OpenShift blog. \nIn addition:\nYour cluster must have one and only one Helm/Tiller instance. You can\ninitialize a Helm/Tiller installation on any host that can access the OpenShift\ncluster.\nThe location of the Helm binary must be set in the PATH environment\nvariable.\nThe helm list command must work.\nThe Tiller service account in the Tiller installation namespace must have the\ncluster-admin role:oc adm policy add-cluster-role-to-user cluster-admin system:serviceaccount:\ntiller-namespace:tiller \n Replace tiller-namespace with the namespace where Tiller is installed. \n \nAvailable resources\nThe following minimum recommendations are for the Cloud Pak for Data\ninstallation.  \nThe time on all of the nodes must be synchronized within 500ms. \nNode\ntype\nHardware Number\nof server\nAvailable\nVPCs\nMemory Storage\n60\nMaster x86-64 1 master\nnode + 1\nextra\nnode for\nhigh\navailabilit\ny.\n16 virtual\nprocessor\ncores\n(VPCs)\n64 GB\nmemory\nYour\ncluster\nmust\nhave\neither:A\nstorage\nclass that\ncan\ncreate a\npersistent\nvolume\nwith at\nleast 800\nGB of\navailable\nspace\nand bind\nthe PV to\na\npersistent\nvolume\nclaim.An\nexternal\nNFS\nserver\nwith 800\nGB of\navailable\nspace.\nThe\nserver\ncannot\nhave\nsquash\nenabled.\nAll of the\nnodes in\nthe\ncluster\nmust\nhave\naccess to\nmount\nthe NFS\nserver\nand have\nread/write\naccess to\nthe\nserver.\n61\n-\n-\nSecurity requirements\nA Docker registry must be accessible from all of the nodes in the cluster.\nAdditionally, all of the nodes must have permission to push to and pull from\nthe registry.\nWorker x86-64 3+ worker\nnodes.Th\ne nodes\nmust be\nable to\nschedule\npods.\n16 virtual\nprocessor\ncores\n(VPCs)\n64 GB\nmemory\nYour\ncluster\nmust\nhave\neither:A\nstorage\nclass that\ncan\ncreate a\npersistent\nvolume\nwith at\nleast 800\nGB of\navailable\nspace\nand bind\nthe PV to\na\npersistent\nvolume\nclaim.An\nexternal\nNFS\nserver\nwith 800\nGB of\navailable\nspace.\nThe\nserver\ncannot\nhave\nsquash\nenabled.\nAll of the\nnodes in\nthe\ncluster\nmust\nhave\naccess to\nmount\nthe NFS\nserver\nand have\nread/write\naccess to\nthe\nserver.\n62\n-\n-\n-\n-\nYou must have a cluster-admin account to install Cloud Pak for Data.Ensure\nthat you are logged in. For example:oc login \n \nWhen you install Cloud Pak for Data you must use the following security\ncontext constraint: allowHostDirVolumePlugin: false\nallowHostIPC: true\nallowHostNetwork: false\nallowHostPID: false\nallowHostPorts: false\nallowPrivilegedContainer: false\nallowedCapabilities:\n- '*'\nallowedFlexVolumes: null\napiVersion: v1\ndefaultAddCapabilities: null\nfsGroup:\n type: RunAsAny\ngroups:\n- cluster-admins\nkind: SecurityContextConstraints\nmetadata:\n annotations:\n   kubernetes.io/description: zenuid provides all features of the restricted SCC but allows users to\nrun with any UID and any GID.\n name: zenuid\npriority: 10\nreadOnlyRootFilesystem: false\nrequiredDropCapabilities: null\nrunAsUser:\n type: RunAsAny\nseLinuxContext:\n type: MustRunAs\nsupplementalGroups:\n type: RunAsAny\nusers: []\nvolumes:\n- configMap\n- downwardAPI\n- emptyDir\n- persistentVolumeClaim\n- projected\n- secret \nLinux configuration requirements\nLog in to each node in the cluster as root and permanently set the Linux\nkernel semaphore parameter:echo \"kernel.sem = 250 1024000 32 4096\" >> /etc/sysctl.conf\n;sysctl -p \n \n Procedure \n63\n1.\nA.\n-\n-\nB.\nC.\n-\n-\n2.\n3.\n \nTo install Cloud Pak for Data on OpenShift: \nObtain the Cloud Pak for Data installation file:\nDownload the installation file for an existing IBM Cloud Private installation\nfrom IBM Passport :\nEnterprise Edition: ICP4D_ENT_Req_ICP_x86_Vnnn.bin\nCloud Native Edition: ICP4D_CNE_Req_ICP_x86_Vnnn.bin\n \nMake the BIN file executable. For example:chmod +x ICP4D_ENT_Req_ICP_x86_Vnnn.bin \n \nWhen prompted, enter y to download the installation file.The BIN file\ndownloads installation file:\nEnterprise Edition: /ibm/ICP4D/icp4d_ee_pre_nnn_x86_64.tar\nCloud Native Edition: /ibm/ICP4D/icp4d_cne_pre_nnn_x86_64.tar\n \nCreate a project for Cloud Pak for Data named zen and switch to the project.oc\nnew-project zen\noc project zen \nFrom this point forward, zen is referred to as the Cloud Pak for Data\nnamespace. \n \nRun the following command to create the required security context constraint:oc\ncreate -f - << EOF\nallowHostDirVolumePlugin: false\nallowHostIPC: true\nallowHostNetwork: false\nallowHostPID: false\nallowHostPorts: false\nallowPrivilegedContainer: false\nallowedCapabilities:\n- '*'\nallowedFlexVolumes: null\napiVersion: v1\ndefaultAddCapabilities: null\nfsGroup:\n type: RunAsAny\ngroups:\n- cluster-admins\nkind: SecurityContextConstraints\nmetadata:\n annotations:\n   kubernetes.io/description: zenuid provides all features of the restricted SCC but allows users to run\nwith any UID and any GID.\n name: zenuid\npriority: 10\nreadOnlyRootFilesystem: false\nrequiredDropCapabilities: null\nrunAsUser:\n64\n4.\n5.\n6.\n7.\n8.\n9.\n10.\nA.\n-\n-\nB.\n-\n-\n type: RunAsAny\nseLinuxContext:\n type: MustRunAs\nsupplementalGroups:\n type: RunAsAny\nusers: []\nvolumes:\n- configMap\n- downwardAPI\n- emptyDir\n- persistentVolumeClaim\n- projected\n- secret\nEOF\noc adm policy add-scc-to-user zenuid system:serviceaccount:zen:default\noc adm policy add-scc-to-user anyuid system:serviceaccount:zen:icpd-anyuid-sa \n \nCreate a cluster role binding and bind it to the default service account in the zen\nnamespace:kubectl create clusterrolebinding admin-on-zen --clusterrole=admin --\nuser=system:serviceaccount:zen:default  -n zen \n \nLog into the Docker registry.docker login Docker-registry-URL -u <username> -p <password> \nReplace Docker-registry-URL with the URL of your Docker registry. \nIf the cluster uses a registry hosted by OpenShift: \ndocker login Docker-registry-URL -u $(oc whoami) -p $(oc whoami -t) \n \nCreate the docker secret for an icp4d-anyuid service account:kubectl create\nsecret -n YOUR_NAMESPACE docker-registry icp4d-anyuid-docker-pull --docker-server=DOCKER_REGISTRY_URL --\ndocker-username=DOCKER_REGISTRY_USER --docker-password=DOCKER_REGISTRY_PASSWORD \nReplace YOUR_NAMESPACE with your IBM Cloud Private namespace. \n \nChange to the directory where the Cloud Pak for Data installation file was\ndownloaded (/ibm/ICP4D) \nRun the following command to start the installation:./installer_filename \n \nEnter A to agree to the terms and conditions.You might need to wait several\nminutes while the installation program extracts required packages. \n \nRespond to the following installation prompts:\nSpecify whether you want to use the default namespace (zen):\nEnter Y to accept the default namespace.\nEnter N to specify a different namespace. Follow the on-screen instructions\nto set up a new namespace. \n \nSpecify whether you want to use the default console port (31843):\nEnter Y to accept the default console port.\nEnter N to specify a different console port.\n65\nC.\nD.\n-\n-\n-\n-\nE.\n-\n-\n-\n-\nF.\n-\n-\n \nSpecify the cluster Docker image prefix, for example\nmycluster.icp:8500/zen, and press Enter.This is the address that your\nnodes use to access the Docker registry. \nSpecify whether the external Docker image prefix is the same as the cluster\nDocker image prefix. For example, you might specify a different external\nDocker image prefix if you are performing the installation from a host that is\nexternal to your cluster. \nEnter Y to use the cluster Docker image prefix as the external Docker.\nEnter N to specify a different external Docker image prefix. You are\nprompted to enter the address of your Docker registry. The address can\nhave one of the following formats:\nhostname\nhostname:port_number\n \nSpecify whether you want to use the default storage class (oketi-gluster):\nEnter Y to accept the default storage class.\nEnter N to select a different storage class. If you don't have an appropriate\nstorage class, the installation program can use an NFS server for the\nstorage and create the required persistent volumes. However, you must\nhave an existing NFS server.Microsoft Azure only: Always enter N to select\nthe NFS entry. \nTo select an existing storage class, specify the number that corresponds to\nthe storage class in the returned list. For example:\n \nTo use an NFS server, enter the number that corresponds to the NFS\nentry in the returned list and specify the IP address and path (mount point)\nof your NFS server. The installation program will use this NFS server for\nstorage and create the required persistent volumes on it.\nRestriction: The No dynamic provisioning option is not recommended. \n \nReview the installation parameters:\nIf the values are correct, enter Y to proceed with the installation.\nIf the values are incorrect, enter N to restart the installation prompts.\nImportant: Invalid values can cause errors during installation.  \n \n Troubleshooting tip: If the token for the docker registry expires and the image can\nno longer pull, enter the following command to update the pull secret:oc delete\nsecret/sa-zen\noc get secret | grep -Eo 'default-dockercfg[^ ]*' | xargs -n 1 oc get secret -o yaml | sed 's/default-\ndockercfg[^ ]*/sa-zen/g' | oc create -f - \n \n \n66\n-\n What to do next \n \nAfter you install Cloud Pak for Data, you must complete the following tasks:\nComplete Setting up the Cloud Pak for Data web client.\n \n67\n"
      ],
      "document_passages": [
        {
          "passage_text": "The\nserver\ncannot\n<em>have</em>\nsquash\nenabled.\nAll of the\nnodes in\nthe\ncluster\nmust\n<em>have</em>\naccess to\nmount\nthe NFS\nserver\nand <em>have</em>\nread/write\naccess to\nthe\nserver.\n62\n-\n-\n-\n-\nYou must <em>have</em> a cluster-admin account to install Cloud Pak for Data.Ensure\nthat you are logged in.",
          "start_offset": 2284,
          "end_offset": 2547,
          "field": "text"
        },
        {
          "passage_text": "The\nserver\ncannot\n<em>have</em>\nsquash\nenabled.\nAll of the\nnodes in\nthe\ncluster\nmust\n<em>have</em>\naccess to\nmount\nthe NFS\nserver\nand <em>have</em>\nread/write\naccess to\nthe\nserver.\n61\n-\n-\nSecurity requirements\nA Docker registry must be accessible from all of the nodes in the cluster.",
          "start_offset": 1594,
          "end_offset": 1851,
          "field": "text"
        }
      ]
    },
    {
      "document_id": "12cefddd-6e95-40d7-8a33-f6efa9d95577",
      "result_metadata": {
        "collection_id": "fbafd5e1-9cf1-c24c-0000-016eeb0a5c9f",
        "document_retrieval_source": "search",
        "confidence": 0.05165
      },
      "enriched_text": [
        {
          "entities": [
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 113,
                    "begin": 110
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 1557,
                    "begin": 1554
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 1770,
                    "begin": 1767
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 1846,
                    "begin": 1843
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 1922,
                    "begin": 1919
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 2126,
                    "begin": 2123
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 2153,
                    "begin": 2150
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 4316,
                    "begin": 4313
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 4575,
                    "begin": 4572
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 4630,
                    "begin": 4627
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 4969,
                    "begin": 4966
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 7988,
                    "begin": 7985
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 10140,
                    "begin": 10137
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 17145,
                    "begin": 17142
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 17177,
                    "begin": 17174
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 17244,
                    "begin": 17241
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 27431,
                    "begin": 27428
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 27463,
                    "begin": 27460
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 27530,
                    "begin": 27527
                  },
                  "text": "IBM"
                }
              ],
              "text": "IBM",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 2121,
                    "begin": 2112
                  },
                  "text": "GlusterFS"
                }
              ],
              "text": "GlusterFS",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 2631,
                    "begin": 2619
                  },
                  "text": "Secure Shell"
                }
              ],
              "text": "Secure Shell",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 4928,
                    "begin": 4921
                  },
                  "text": "2.1.0.2"
                },
                {
                  "location": {
                    "end": 10441,
                    "begin": 10434
                  },
                  "text": "2.1.0.2"
                }
              ],
              "text": "2.1.0.2",
              "type": "IPAddress"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 5017,
                    "begin": 4999
                  },
                  "text": "Red Hat Enterprise"
                }
              ],
              "text": "Red Hat Enterprise",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 5779,
                    "begin": 5776
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 7496,
                    "begin": 7493
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 14053,
                    "begin": 14050
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 14215,
                    "begin": 14212
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 14583,
                    "begin": 14580
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 14613,
                    "begin": 14610
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 14733,
                    "begin": 14730
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 21475,
                    "begin": 21472
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 21744,
                    "begin": 21741
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 21857,
                    "begin": 21854
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 21925,
                    "begin": 21922
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 22580,
                    "begin": 22577
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 22746,
                    "begin": 22743
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 22997,
                    "begin": 22994
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 23065,
                    "begin": 23062
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 23150,
                    "begin": 23147
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 23255,
                    "begin": 23252
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 24594,
                    "begin": 24591
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 24863,
                    "begin": 24860
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 24976,
                    "begin": 24973
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 25044,
                    "begin": 25041
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 25566,
                    "begin": 25563
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 25674,
                    "begin": 25671
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 25784,
                    "begin": 25781
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 25842,
                    "begin": 25839
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 25927,
                    "begin": 25924
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 26044,
                    "begin": 26041
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 29687,
                    "begin": 29684
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 29779,
                    "begin": 29776
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 29804,
                    "begin": 29801
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 29854,
                    "begin": 29851
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 30196,
                    "begin": 30193
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 30431,
                    "begin": 30428
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 31871,
                    "begin": 31868
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 31967,
                    "begin": 31964
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 32356,
                    "begin": 32353
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 32562,
                    "begin": 32559
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 32756,
                    "begin": 32753
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 33110,
                    "begin": 33107
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 34261,
                    "begin": 34258
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 34605,
                    "begin": 34602
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 34749,
                    "begin": 34746
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 35174,
                    "begin": 35171
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 35242,
                    "begin": 35239
                  },
                  "text": "ibm"
                }
              ],
              "text": "ibm",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 7699,
                    "begin": 7696
                  },
                  "text": "Pak"
                },
                {
                  "location": {
                    "end": 8398,
                    "begin": 8395
                  },
                  "text": "Pak"
                }
              ],
              "text": "Pak",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 11576,
                    "begin": 11570
                  },
                  "text": "Docker"
                }
              ],
              "text": "Docker",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 13419,
                    "begin": 13408
                  },
                  "text": "mount point"
                }
              ],
              "text": "mount point",
              "type": "GeographicFeature"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 14676,
                    "begin": 14673
                  },
                  "text": "tls"
                }
              ],
              "text": "tls",
              "type": "PrintMedia"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 17972,
                    "begin": 17946
                  },
                  "text": "Hadoop Integration service"
                },
                {
                  "location": {
                    "end": 18326,
                    "begin": 18300
                  },
                  "text": "Hadoop Integration service"
                }
              ],
              "text": "Hadoop Integration service",
              "type": "Organization"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 18510,
                    "begin": 18506
                  },
                  "text": "8 GB"
                }
              ],
              "text": "8 GB",
              "type": "Quantity"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 18536,
                    "begin": 18530
                  },
                  "text": "100 GB"
                }
              ],
              "text": "100 GB",
              "type": "Quantity"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 18847,
                    "begin": 18842
                  },
                  "text": "10 GB"
                }
              ],
              "text": "10 GB",
              "type": "Quantity"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 18917,
                    "begin": 18913
                  },
                  "text": "1\nGB"
                }
              ],
              "text": "1\nGB",
              "type": "Quantity"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 19785,
                    "begin": 19781
                  },
                  "text": "jobs"
                }
              ],
              "text": "jobs",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 20519,
                    "begin": 20515
                  },
                  "text": "Livy"
                },
                {
                  "location": {
                    "end": 31664,
                    "begin": 31660
                  },
                  "text": "Livy"
                }
              ],
              "text": "Livy",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 21498,
                    "begin": 21493
                  },
                  "text": "chown"
                },
                {
                  "location": {
                    "end": 21598,
                    "begin": 21593
                  },
                  "text": "chown"
                },
                {
                  "location": {
                    "end": 21695,
                    "begin": 21690
                  },
                  "text": "chown"
                },
                {
                  "location": {
                    "end": 24749,
                    "begin": 24744
                  },
                  "text": "chown"
                }
              ],
              "text": "chown",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 21983,
                    "begin": 21979
                  },
                  "text": "livy"
                },
                {
                  "location": {
                    "end": 22055,
                    "begin": 22051
                  },
                  "text": "livy"
                },
                {
                  "location": {
                    "end": 25102,
                    "begin": 25098
                  },
                  "text": "livy"
                },
                {
                  "location": {
                    "end": 25174,
                    "begin": 25170
                  },
                  "text": "livy"
                }
              ],
              "text": "livy",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 22050,
                    "begin": 22045
                  },
                  "text": "dsxhi"
                },
                {
                  "location": {
                    "end": 25169,
                    "begin": 25164
                  },
                  "text": "dsxhi"
                }
              ],
              "text": "dsxhi",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 25319,
                    "begin": 25307
                  },
                  "text": "bin/su dsxhi"
                }
              ],
              "text": "bin/su dsxhi",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 25425,
                    "begin": 25417
                  },
                  "text": "su dsxhi"
                }
              ],
              "text": "su dsxhi",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 27027,
                    "begin": 27018
                  },
                  "text": "Cloud Pak"
                }
              ],
              "text": "Cloud Pak",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 27756,
                    "begin": 27753
                  },
                  "text": "JWT"
                }
              ],
              "text": "JWT",
              "type": "Organization"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 29385,
                    "begin": 29378
                  },
                  "text": "keytabs"
                }
              ],
              "text": "keytabs",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 29753,
                    "begin": 29743
                  },
                  "text": "sudo chown"
                }
              ],
              "text": "sudo chown",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 33315,
                    "begin": 33308
                  },
                  "text": "Alluxio"
                },
                {
                  "location": {
                    "end": 33518,
                    "begin": 33511
                  },
                  "text": "Alluxio"
                }
              ],
              "text": "Alluxio",
              "type": "Person"
            }
          ]
        }
      ],
      "metadata": {
        "parent_document_id": "12cefddd-6e95-40d7-8a33-f6efa9d95577"
      },
      "extracted_metadata": {
        "sha1": "4D7E4A554FC0B99B93FDFAF90C3FE11357C6CF15",
        "numPages": "18",
        "filename": "ovu-78-95.pdf",
        "file_type": "pdf"
      },
      "text": [
        "1.\n2.\n3.\n4.\n5.\n6.\n Testing the load balancer \nIt is good practice to test the load balancer that is used with IBMÂ® Cloud Pak for\nData. \n Procedure \n \nVerify that you have the ncat utility installed on all master nodes by using the\nwhich ncat command.If the command returns nothing, install the ncat\npackage on your master nodes with the command: yum install -y nc \n \nOn each master node, use ncat to listen for connections on the port that you are\ntesting by running the following command:ncat -lkp port_number \nThe command doesn't return a result because the ncat utility is waiting for an\nincoming connection on the specified port_number. \n \nOn a different Linux system with a connection to the load balancer system, run\nthe command:echo 1 | ncat load-balancer-ip-addressport_number \n \nOn one of the master nodes, the ncat utility prints a 1 on the screen. \n \nEach time that you run the command, a 1 is randomly printed on one of the\nmaster nodes, as determined by the load balancer.Make sure to test all ports that\nare used by Cloud Pak for Data. The required ports are 80, 443, 8001, 8443,\n8600, 9443, and 31843. \nWhen you confirm that all master nodes are able to print 1, stop the ncat\nprocess on the master nodes with the Ctrl+C keyboard combination.\nIf you do not see the expected results, verify the load balancer configuration, and\nmake sure that you are not blocked by a firewall or proxy.\n \nParent topic:Post-installation tasks \n \n78\n-\n-\n-\n-\n-\n-\n Securing communication ports \nTo ensure secure transmission of network traffic to and from the IBMÂ® Cloud Pak\nfor Data cluster, you need to configure the communication ports used by the\nnetwork. Use the following information to determine which specific ports to configure\nfor your cloud platform. \n Existing IBM Cloud Private environments \nIf you are installing Cloud Pak for Data on IBM Cloud Private, see the official\ndocumentation on the required ports for IBM Cloud Private. The references in that\ndocument to the Management, Proxy, and Master nodes, all correspond to the\nCloud Pak for Data master node. Also, you do not need to open the ports for\nGlusterFS, IBM Multicloud Manager, or IBM Edge Computing for Servers. \n Installation of Cloud Pak for Data \nWhen you install Cloud Pak for Data, either as a stand-alone deployment or on top\nof an existing cluster, additional ports need to be opened for the nodes to\ncommunicate properly.  \nUse the security or network management interface specific to your cloud platform.\nFor each of the following ports, create a security rule that allows inbound and\noutbound connections through the firewall:\n22\nPort for Secure Shell (SSH) connections. Required during installation for\ncommunications between cluster nodes\n31843\nPort used by the default instance of Cloud Pak for Data, for connections to the\nCloud Pak for Data web client and for REST API clients\n \nAfter the Cloud Pak for Data installation process is complete, configure your firewall\nfor maximum security of your connection ports. Allow all outbound traffic from the\ncluster, but allow inbound traffic only on the port range that is required for clients to\ncommunicate with the Cloud Pak for Data cluster:\n30000-32767\nExternal port range used by the cluster when mapping internal cluster services\nto external facing ports.This port range includes port 31843, the port used by\nthe default instance of Cloud Pak for Data for connections to the web client and\nfor REST API clients. Each additional instance on the Cloud Pak for Data\ncluster has a unique default port for those connections. \n \n Ports for add-ons \nWhen you provision a new add-on or integration on your Cloud Pak for Data cluster,\nthe services might require connections to be made from outside the cluster. For\nexample, when you access databases, or run data virtualization through an\nODBC/JDBC connection. If the add-on or integration requires connections to be\nmade to the cluster, then you need to open the appropriate network ports. Refer to\nthe detailed information for your specific add-on or integration to determine what\nextra connections might need to be configured. \n Securing Transport Layer Security ports \nAfter you install Cloud Pak for Data, it is  recommended that you complete the steps\nfor Configuring TLS and cipher suites for the image manager and registry in the\nIBM Cloud Private documentation. If you do not complete this task, TLS ports are\n79\nvulnerable through TLS 1.0/1.1. \n \nParent topic:Post-installation tasks \n \n80\n1.\n2.\n3.\nA.\nB.\nC.\nD.\n-\n-\n-\n-\nE.\n Installing multiple instances of Cloud Pak for Data\non the same IBM Cloud Private cluster \nAfter you finish installing IBMÂ® Cloud Pak for Data on a cluster, you can reuse the\nsame specifications to install another instance of the same Cloud Pak for Data\nrelease on the same cluster. This new instance must use a different namespace and\nport number. \n Before you begin \n \nRequirement: A Cloud Pak for Data Version 2.1.0.2 or later installation in an existing\nIBM Cloud Private environment on Red Hat Enterprise Linux using NFS storage\nonly. Ensure the cluster has enough resource for the new instance. \n About this task \n \nCloud Pak for Data supports a multitenancy architecture where different areas of\nyour organization can share the same cluster, yet operate independently in their\nown dedicated Cloud Pak for Data instances. Each instance has its own isolated\nusers, data, quotas, namespace and ports. \n Procedure \n \nCreate a namespace (cannot exceed eight characters) and docker secret for the\nnew Cloud Pak for Data instance:kubectl create namespace NEW_NAMESPACE \nkubectl create secret -n NEW_NAMESPACE docker-registry icp4d-anyuid-docker-pull --docker-server=\nDOCKER_REGISTRY_URL --docker-username=admin --docker-password=DOCKER_REGISTRY_PASSWORD \n \nGo to the /ibm/InstallPackages/components directory and run ./installer.sh.\nRespond to the following installation prompts:\nSpecify the new namespace you created.\nSpecify a different console port from your previous installation.The valid port\nrange is 30000-32767. \nSpecify the cluster Docker image prefix, for example\nmycluster.icp:8500/zen, and press Enter.This is the address that your\nnodes use to access the Docker registry. \nSpecify whether the external Docker image prefix is the same as the cluster\nDocker image prefix. For example, you might specify a different external\nDocker image prefix if you are performing the installation from a host that is\nexternal to your cluster. \nEnter Y to use the cluster Docker image prefix as the external Docker.\nEnter N to specify a different external Docker image prefix. You are prompted\nto enter the address of your Docker registry. The address can have one of\nthe following formats:\nhostname\nhostname:port_number\n \nEnter N to select a different storage class from the default.\n \n81\nF.\nG.\n-\n-\n4.\n-\n1.\n2.\n3.\n4.\n \n \nEnter the number that corresponds to the NFS entry in the returned list.\nSpecify the IP address and path (mount point) of your NFS server. The\ninstaller will use this NFS server for storage and create the required persistent\nvolumes on it.\nReview the installation parameters:\nIf the values are correct, enter Y to proceed with the installation.\nIf the values are incorrect, enter N to restart the installation prompts.\nImportant: Invalid values can cause errors during installation.  \n \nAfter the installation of the new instance completes, enter the following command\nand verify that the new Cloud Pak for Data namespace exists:kubectl get svc--all-\nnamespaces|grep ibm-nginx \nCopy the port name for the client URL so that you can sign into the new instance\nof the client. \n What to do next \n \nRecommendation: Set a resource quota on each namespace to ensure the Cloud\nPak for Data instances cannot overuse the CPU, memory, and storage resources on\nthe cluster. See Setting resource quota for instructions. \n After you install the new Cloud Pak for Data instance, you must complete the\nfollowing tasks:\nComplete Setting up the Cloud Pak for Data web client.\nIBM Cloud Private restriction: When accessing a Cloud Pak for Data instance,\nensure you sign out before you access a different instance on a different port. Using\nthe same browser session (for example, in different browser tabs) can result in\nauthorization failures. Alternatively, you can use a different load balancer host for\neach Cloud Pak for Data instance or use a DNS-based route to identify each Cloud\nPak for Data instance (where each instance is identified by a different host name but\ncan still point to the same proxy or load balancer IP address). Note that this\nrestriction does not apply to OpenShift because it uses routes. \nData governance troubleshooting tip: If the data governance feature in the new\nCloud Pak for Data instance stalls on iis-xmetarepro and this pod is not coming\nup, then check the nodes on which iis-xmetarepro pods of both instances are\nscheduled to see whether they are scheduled on same node. For each namespace\nenter the following command:kubectl get po -o wide -n namespace | grep iis-xmetarepro \nIf both pods are scheduled on the same node, then complete the following\nworkaround to schedule the iis-xmetarepro pod on a different node:\nLabel the different node to schedule the iis-xmetarepro pod on using the\nxmeta= instance2 key and value pair:kubectl label node node_IP xmeta= instance2 \nVerify that the label is now on the node: kubectl get nodes --show-labels \nEdit the deployment: kubectl get deploy -n namespace | grep iis-xmeta\nkubectl edit deploy -n namespacedeployment_name \nIn the node affinity section, insert the key and value under\nrequiredDuringSchedulingIgnoredDuringExecution:. Example:\nnodeSelectorTerms:\n- matchExpressions:\n82\n5.\n6.\n - key: xmeta\n   operator: In\n   values:\n   - instance2 \nSave the changes. The node should now automatically terminate the old iis-\nxmetarepro pod and schedule a new one.\nVerify that the iis-xmetarepro pod is scheduled on a different node and is up\nand running.\n \n \nParent topic:Post-installation tasks \n \n83\n1.\n2.\n3.\n4.\n5.\nA.\nB.\nC.\nD.\n-\n Installing multiple instances of Cloud Pak for Data\non the same Red Hat OpenShift cluster \nAfter you finish installing IBMÂ® Cloud Pak for Data on a cluster, you can reuse the\nsame specifications to install another instance of the same Cloud Pak for Data\nrelease on the same Red Hat OpenShift cluster. This new instance must use a\ndifferent namespace.  \n Before you begin \n \nRequirement: A Cloud Pak for Data Version 2.1.0.2 or later installation in an existing\nOpenShift environment. Ensure the cluster has enough resource for the new\ninstance, and ensure the cluster has only one Tiller pod. \n About this task \n \nCloud Pak for Data supports a multitenancy architecture where different areas of\nyour organization can share the same cluster, yet operate independently in their\nown dedicated Cloud Pak for Data instances. Each instance has its own isolated\nusers, data, quotas, namespace and ports. \n Procedure \n \nCreate another project for Cloud Pak for Data with a new namespace and switch\nto the project.oc new-project new_namespace \noc project new_namespace \nReplace new_namespace with the new namespace you want to use for the new\ninstance, for example, zen2. \n \nExport the TILLER_NAMESPACE environment variable with the Tiller\nnamespace from your first installation:export TILLER_NAMESPACE=tiller_namespace \n \nLog into the Docker registry:docker login Docker-registry-URL -u $(oc whoami) -p $(oc whoami -t) \nReplace Docker-registry-URL with the URL of your Docker registry. \nThe following example command checks the docker login for the integrated\nDocker registry that is hosted by OpenShift: \ndocker login docker-registry.default.svc:5000 -u ocadmin -p $(oc whoami -t) \n \nGo to the INSTALL_HOME/InstallPackages/components directory and run\n./installer.sh.\nRespond to the following installation prompts:\nSpecify the new namespace you created.\nSpecify a different console port from your previous installation.The valid port\nrange is 30000-32767. \nSpecify the cluster Docker image prefix, for example\nmycluster.icp:8500/zen, and press Enter.This is the address that your\nnodes use to access the Docker registry. \nSpecify whether the external Docker image prefix is the same as the cluster\nDocker image prefix. For example, you might specify a different external\nDocker image prefix if you are performing the installation from a host that is\nexternal to your cluster. \nEnter Y to use the cluster Docker image prefix as the external Docker.\n84\n-\n-\n-\nE.\n-\n-\n-\n-\nF.\n-\n-\n6.\n7.\n8.\nA.\nB.\nC.\nD.\n9.\nEnter N to specify a different external Docker image prefix. You are\nprompted to enter the address of your Docker registry. The address can\nhave one of the following formats:\nhostname\nhostname:port_number\n \nSpecify whether you want to use the default storage class (oketi-gluster):\nEnter Y to accept the default storage class.\nEnter N to select a different storage class. If you don't have an appropriate\nstorage class, the installation program can use an NFS server for the\nstorage and create the required persistent volumes. However, you must\nhave an existing NFS server.Microsoft Azure only: Always enter N to select\nthe NFS entry. \nTo select an existing storage class, specify the number that corresponds to\nthe storage class in the returned list. For example:\n \nTo use an NFS server, enter the number that corresponds to the NFS\nentry in the returned list and specify the IP address and path (mount point)\nof your NFS server. The installation program will use this NFS server for\nstorage and create the required persistent volumes on it.\nRestriction: The No dynamic provisioning option is not recommended. \n \nReview the installation parameters:\nIf the values are correct, enter Y to proceed with the installation.\nIf the values are incorrect, enter N to restart the installation prompts.\nImportant: Invalid values can cause errors during installation.  \n \nAfter the installation of the new instance completes, enter the following\ncommand and verify that the new Cloud Pak for Data namespace exists:kubectl\nget svc--all-namespaces|grep ibm-nginx \n \nRetrieve the OpenShift routes:oc project new_namespace \noc get svc | grep nginx \n \nEdit the OpenShift route:\nEnter the following command:oc edit svc ibm-nginx-svc \n \nFor spec, remove the nodePort entry, for example, nodePort:31863.\nChange type from NodePort to ClusterIP.\nSave the file.\nCreate a new YAML file with the following lines. For host, type in your\nnamespace and hostname for the new Cloud Pak for Data instance:apiVersion:\nroute.openshift.io/v1\n85\n10.\n11.\n12.\n13.\n-\n1.\n2.\n3.\n4.\nkind: Route\nmetadata:\nname: ibm-nginx-route\nspec:\n  host: ibm-nginx-<namespace>.apps.<hostname>  \nport:\n targetPort: 443\ntls:\n termination: passthrough\nto:\n kind: Service\n name: ibm-nginx-svc\n weight: 100 \n \nSave the file.\nEnter the following command to create the route:oc create -f ./file.yaml \n \nEnter the following command to get the route:oc get routes \nThe command returns the web address to the web client for the new Cloud Pak\nfor Data instance. \n \nUse the route to access the Cloud Pak for Data web client for the new instance:\nhttps://ibm-nginx-namespace.apps.hostname \nNote that you cannot use hostname:port to access the instance. \n \n What to do next \n \nAfter you install the new Cloud Pak for Data instance, you must complete the\nfollowing tasks:\nComplete Setting up the Cloud Pak for Data web client.\nData governance troubleshooting tip: If the data governance feature in the new\nCloud Pak for Data instance stalls on iis-xmetarepro and this pod is not coming\nup, then check the nodes on which iis-xmetarepro pods of both instances are\nscheduled to see whether they are scheduled on same node. For each namespace\nenter the following command:kubectl get po -o wide -n namespace | grep iis-xmetarepro \nIf both pods are scheduled on the same node, then complete the following\nworkaround to schedule the iis-xmetarepro pod on a different node:\nLabel the different node to schedule the iis-xmetarepro pod on using the\nxmeta= instance2 key and value pair:kubectl label node node_IP xmeta= instance2 \nVerify that the label is now on the node: kubectl get nodes --show-labels \nEdit the deployment: kubectl get deploy -n namespace | grep iis-xmeta\nkubectl edit deploy -n namespacedeployment_name \nIn the node affinity section, insert the key and value under\nrequiredDuringSchedulingIgnoredDuringExecution:. Example:\nnodeSelectorTerms:\n- matchExpressions:\n - key: xmeta\n   operator: In\n86\n5.\n6.\n   values:\n   - instance2 \nSave the changes. The node should now automatically terminate the old iis-\nxmetarepro pod and schedule a new one.\nVerify that the iis-xmetarepro pod is scheduled on a different node and is up\nand running.\n \n \nParent topic:Post-installation tasks \n \n87\n-\n-\n-\n-\n-\n-\n Setting up a remote Hadoop cluster to work with\nCloud Pak for Data \nYou can set up a remote Hadoop cluster to allow Cloud Pak for Data  users to\nsecurely access the data that resides on a Hadoop cluster and submit jobs to use\nthe compute resources on the Hadoop cluster.  \nThe Hadoop integration software is a separately priced feature. To use the\nHadoop integration software, you must purchase the IBMÂ® Watsonâ¢ Studio\nPremium for IBM Cloud Pak for Data package. For more information, contact your\nIBM Sales representative.  \nCloud Pak for Data interacts with a Hadoop cluster through the following four\nservices:\n \nSetting up a Hadoop cluster for Cloud Pak for Data entails installing and configuring\nthe four services. Additionally, for kerberized clusters, the setup entails configuring a\ngateway with JWT-based authentication to securely authenticate requests from\nCloud Pak for Data users. Important: The set up tasks must be performed by a\nHadoop administrator. \n \n Security and authentication for Hadoop \nThe Cloud Pak for Data Hadoop registration service eases the setup of a Hadoop\ncluster for Cloud Pak for Data, and gives additional functionality of scheduling jobs\nas YARN application. \nThe Hadoop Integration service is a secure service installed on the edge node of the\nHadoop cluster. Access to the service is restricted to an explicit list of Cloud Pak for\nData clusters via a secure URL. Every request from Cloud Pak for Data includes the\nJWT token of the signed in Cloud Pak for Data user, and is authenticated against\nthe secure URL. The Hadoop Integration service extracts the username from the\nJWT token and propagates the username for all data access and job submission on\nthe Hadoop cluster.  \nEdge node requirements\nHardware requirements:\n8 GB memory\n2 CPU cores\n100 GB disk, mounted and available on /var in the local Linux file system. The\ninstallation creates the following directories: /var/log/dsxhi, /var/log/livy, and\n/var/log/livy2 to store the logs and /var/run/dsxhi, /var/run/livy and\n/var/run/livy2 to store the process IDs. These locations are not configurable.\n10 GB network interface card recommended for multi-tenant environment (1\nGB network interface card if WEBHDFS will not be heavily utilized)\nService Purpose\nWebHDFS Browse and preview HDFS data\nWebHCAT Browse and preview Hive tables\nLivy for Spark Submit jobs to Spark on the\nHadoop cluster.\nLivy for Spark2 Submit jobs to Spark2 on the\nHadoop cluster.\n88\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n In addition, the edge node must meet the following requirements:\nHave Python 2.7 installed.\nCDH only: Have Java Development Kit Version 1.8 installed.\nHave curl 7.19.7-53 or later to allow secure communication between the\nHadoop registration service and Cloud Pak for Data.\nHave a service user that can run the Hadoop registration service. This user\nshould be a valid Linux user with a home directory created in HDFS.\nThe service user should have the necessary Hadoop Proxyuser privileges in\nHDFS, WebHCAT and Livy services to access data and submit asynchronous\njobs as Cloud Pak for Data users. HDP only: If Hadoop or Ranger KMS is\nenabled, the service user should have necessary proxyuser privileges in kms-\nsite.xml.\nFor a kerberized cluster: Have the keytab for the service user. This\neliminates the need for every Cloud Pak for Data user to have a valid keytab.\nHave an available port for the Hadoop registration service. The port for the\nHadoop registration service should be exposed for access from the Cloud Pak\nfor Data clusters that need to connect to the HDP cluster.\nHave an available port for the Hadoop registration Rest service. This port\nneed not be exposed for external access.\nDepending on the service that needs to be exposed by Hadoop registration,\nhave an available port for Livy for Spark and Livy for Spark 2. These ports do\nnot need to be exposed for external access.\nWhen submitting jobs to a Hadoop cluster where Kerberos security is not\nenabled, the Yarn user should have write access to the directories accessed\nby the job.\n Requirements for a service user installing the Hadoop\nregistration service \nIf you plan to install the Hadoop registration service as a service user rather than as\nroot, you must first use the visudo command to be add the following access\ninformation in /etc/sudoers: \n## DSXHI - General Installation (replace <service_user>)\n<service_user> ALL=(root) NOPASSWD: /usr/bin/yum install dsxhi*, /usr/bin/yum install wshi*, /usr/bin/yum\nerase dsxhi*, /usr/bin/mkdir -p /etc/dsxhi, /usr/bin/mkdir -p /var/log/dsxhi, /usr/bin/mkdir -p\n/var/run/dsxhi, /usr/bin/mkdir -p /var/log/livy, /usr/bin/mkdir -p /var/run/livy, /usr/bin/mkdir -p\n/var/log/livy2, /usr/bin/mkdir -p /var/run/livy2, /usr/bin/chown * /opt/ibm/dsxhi/, /usr/bin/chown *\n/etc/dsxhi/conf, /usr/bin/chown * /var/log/dsxhi, /usr/bin/chown * /var/run/dsxhi, /usr/bin/chown *\n/var/log/livy, /usr/bin/chown * /var/run/livy, /usr/bin/chown * /var/log/livy2, /usr/bin/chown *\n/var/run/livy2, /usr/bin/chmod 400 -R /opt/ibm/dsxhi/security/*, /usr/bin/chmod 755 /var/log/dsxhi,\n/usr/bin/chmod 755 /var/run/dsxhi, /usr/bin/ln -sf /opt/ibm/dsxhi/gateway/logs /var/log/dsxhi/gateway,\n/usr/bin/ln -sf /opt/ibm/dsxhi/conf /etc/dsxhi/conf, /usr/bin/ln -sf /var/log/livy /var/log/dsxhi/livy,\n/usr/bin/ln -sf /var/log/livy2 /var/log/dsxhi/livy2\n \n## DSXHI - Service User Specific Commands (replace <service_user>)\n<service_user> ALL=(root) NOPASSWD:  /usr/bin/su <service_user> -c hdfs dfs -test -e /user/<service_user>/*,\n/usr/bin/su <service_user> -c hdfs dfs -test -d /user/<service_user>/*, /usr/bin/su <service_user> -c hdfs\ndfs -mkdir /user/<service_user>/*, /usr/bin/su <service_user> -c hdfs dfs -chmod 755 /user/<service_user>/*,\n/usr/bin/su <service_user> -c hdfs dfs -chmod 644 /user/<service_user>/*, /usr/bin/su <service_user> -c hdfs\ndfs -put -f /opt/ibm/<service_user>/* /user/<service_user>/*, /usr/bin/su <service_user> -c hdfs\n89\nservice_user -rm -r /user/<service_user>/*,  /usr/bin/su <service_user> -c sh\n/opt/ibm/dsxhi/bin/util/gateway_config.sh *\n \n## DSXHI - Security (only needed if security is enabled; replace <service_user>, and replace\n<service_keytab> with the path to the service user keytab)\n<service_user> ALL=(root) NOPASSWD: /usr/bin/chown * /opt/ibm/dsxhi/security/*, /usr/bin/su <service_user> -\nc kinit -kt /opt/ibm/dsxhi/security/* *, /usr/bin/cp /etc/security/keytabs/spnego.service.keytab\n/opt/ibm/dsxhi/security/*, /usr/bin/su <service_user> -c /usr/bin/kdestroy, /usr/bin/cp <service_keytab>\n/opt/ibm/dsxhi/security/*\n \n## DSXHI - HDP (only needed for HDP)\n<service_user> ALL=(root) NOPASSWD: /usr/sbin/ambari-agent --version, /usr/jdk64/jdk1.8.0_112/bin/keytool -\ndelete -keystore /usr/jdk64/jdk1.8.0_112/jre/lib/security/cacerts *, /usr/jdk64/jdk1.8.0_112/bin/keytool -\nexportcert -file dsxhi_rest.crt -keystore dsxhi_rest.jks *, /usr/jdk64/jdk1.8.0_112/bin/keytool -import -\nfile dsxhi_rest.crt -keystore /usr/jdk64/jdk1.8.0_112/jre/lib/security/cacerts *\n \n## DSXHI - CDH (only needed for CDH)\n<service_user> ALL=(root) NOPASSWD: /usr/java/jdk1.7.0_67-cloudera/bin/keytool -delete -keystore\n/usr/java/jdk1.7.0_67-cloudera/jre/lib/security/cacerts *, /usr/java/jdk1.7.0_67-cloudera/bin/keytool -\nexportcert -file dsxhi_rest.crt -keystore dsxhi_rest.jks *, /usr/java/jdk1.7.0_67-cloudera/bin/keytool -\nimport -file dsxhi_rest.crt -keystore /usr/java/jdk1.7.0_67-cloudera/jre/lib/security/cacerts *\n \nExample of a working, populated configuration for HDP security: \n## DSXHI - General Installation\ndsxhi ALL=(root) NOPASSWD: /usr/bin/yum install dsxhi*, /usr/bin/yum erase dsxhi*, /usr/bin/mkdir -p\n/etc/dsxhi, /usr/bin/mkdir -p /var/log/dsxhi, /usr/bin/mkdir -p /var/run/dsxhi, /usr/bin/mkdir -p\n/var/log/livy, /usr/bin/mkdir -p /var/run/livy, /usr/bin/mkdir -p /var/log/livy2, /usr/bin/mkdir -p\n/var/run/livy2, /usr/bin/chown * /opt/ibm/dsxhi/, /usr/bin/chown * /etc/dsxhi/conf, /usr/bin/chown *\n/var/log/dsxhi, /usr/bin/chown * /var/run/dsxhi, /usr/bin/chown * /var/log/livy, /usr/bin/chown *\n/var/run/livy, /usr/bin/chown * /var/log/livy2, /usr/bin/chown * /var/run/livy2, /usr/bin/chmod 400 -R\n/opt/ibm/dsxhi/security/*, /usr/bin/chmod 755 /var/log/dsxhi, /usr/bin/chmod 755 /var/run/dsxhi, /usr/bin/ln\n-sf /opt/ibm/dsxhi/gateway/logs /var/log/dsxhi/gateway, /usr/bin/ln -sf /opt/ibm/dsxhi/conf /etc/dsxhi/conf,\n/usr/bin/ln -sf /var/log/livy /var/log/dsxhi/livy, /usr/bin/ln -sf /var/log/livy2 /var/log/dsxhi/livy2\n \n## DSXHI - Service User Specific Commands\ndsxhi ALL=(root) NOPASSWD:  /usr/bin/su dsxhi -c hdfs dfs -test -e /user/dsxhi/*, /usr/bin/su dsxhi -c hdfs\ndfs -test -d /user/dsxhi/*, /usr/bin/su dsxhi -c hdfs dfs -mkdir /user/dsxhi/*, /usr/bin/su dsxhi -c hdfs\ndfs -chmod 755 /user/dsxhi/*, /usr/bin/su dsxhi -c hdfs dfs -chmod 644 /user/dsxhi/*, /usr/bin/su dsxhi -c\nhdfs dfs -put -f /opt/ibm/dsxhi/* /user/dsxhi/*, /usr/bin/su dsxhi -c hdfs dfs -rm -r /user/dsxhi/*,\n/usr/bin/su dsxhi -c sh /opt/ibm/dsxhi/bin/util/gateway_config.sh *\n \n## DSXHI - Security\ndsxhi ALL=(root) NOPASSWD: /usr/bin/chown * /opt/ibm/dsxhi/security/*, /usr/bin/su dsxhi -c kinit -kt\n/opt/ibm/dsxhi/security/* *, /usr/bin/cp /etc/security/keytabs/spnego.service.keytab\n/opt/ibm/dsxhi/security/*, /usr/bin/su dsxhi -c /usr/bin/kdestroy, /usr/bin/cp\n/etc/security/svckeytabs/dsxhi.keytab /opt/ibm/dsxhi/security/*\n \n## DSXHI - HDP\n90\ndsxhi ALL=(root) NOPASSWD: /usr/sbin/ambari-agent --version, /usr/jdk64/jdk1.8.0_112/bin/keytool -delete -\nkeystore /usr/jdk64/jdk1.8.0_112/jre/lib/security/cacerts *, /usr/jdk64/jdk1.8.0_112/bin/keytool -exportcert\n-file dsxhi_rest.crt -keystore dsxhi_rest.jks *, /usr/jdk64/jdk1.8.0_112/bin/keytool -import -file\ndsxhi_rest.crt -keystore /usr/jdk64/jdk1.8.0_112/jre/lib/security/cacerts *\n \n## DSXHI - CDH\ndsxhi ALL=(root) NOPASSWD: /usr/java/jdk1.7.0_67-cloudera/bin/keytool -delete -keystore\n/usr/java/jdk1.7.0_67-cloudera/jre/lib/security/cacerts *, /usr/java/jdk1.7.0_67-cloudera/bin/keytool -\nexportcert -file dsxhi_rest.crt -keystore dsxhi_rest.jks *, /usr/java/jdk1.7.0_67-cloudera/bin/keytool -\nimport -file dsxhi_rest.crt -keystore /usr/java/jdk1.7.0_67-cloudera/jre/lib/security/cacerts *\n \n \nParent topic:Post-installation tasks \n \n91\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n  Setting up Hortonworks Data Platform (HDP) to\nwork with Cloud Pak for Data \nCloud Pak for Data provides a Hadoop registration service that eases the setup of\nan HDP cluster for Cloud Pak for Data. Using the Hadoop registration service is the\nrecommended approach, and gives additional functionality of scheduling jobs as\nYARN application. \nThe Hadoop integration software is a separately priced feature. To use the\nHadoop integration software, you must purchase the IBMÂ® Watsonâ¢ Studio\nPremium for IBM Cloud Pak for Data package. For more information, contact your\nIBM Sales representative.  \nSetting up an HDP cluster for Cloud Pak for Data entails installing and configuring\nthe following four services.\n Additionally, for kerberized clusters, the setup entails configuring a gateway with\nJWT-based authentication to securely authenticate requests from Cloud Pak for\nData users.  \nImportant: The following tasks must be performed by a Hadoop administrator. \nSet up a HDP cluster with Hadoop registration\nCreate an edge node\nInstall the Hadoop registration service\nManage the Hadoop registration service\nAdd certificates for SSL enabled services\nManage Cloud Pak for Data for Hadoop registration\nHadoop registration URL for secure access from Cloud Pak for Data\nUninstall the Hadoop registration service\n Supported versions \nHDP Version 2.5.6 and later fixpacks\nHDP Version 2.6.2 and later fixpacks\nHDP Version 3.0.1 and 3.1\n Platforms supported \nHadoop registration is supported on all platforms supported by the HDP versions. \n Set up a HDP cluster with Hadoop registration \nCreate an edge node\nThe Hadoop registration service can be installed on a shared edge node if the\nresources listed above are exclusively available for Hadoop registration. See\nEdge node requirements for the hardware and software requirements. When\nthe edge node is successfully created, it should have the following components:\nThe HDFS Gateway Role and YARN Gateway Role installed.\nThe Spark client installed if the HDP cluster has a Spark service.\nService Purpose\nWebHDFS Browse and preview HDFS data\nWebHCAT Browse and preview Hive tables\nLivy for Spark Submit jobs to Spark on the\nHadoop cluster.\nLivy for Spark2 Submit jobs to Spark2 on the\nHadoop cluster.\n92\n-\n-\n-\n-\n1.\n2.\n3.\n4.\n5.\n-\n-\n-\n-\nThe Spark2 client installed if the HDP cluster has a Spark2 service.\nFor a kerberized cluster, have the spnego keytab copied to\n/etc/security/keytabs/spnego.service.keytab.\nInstall the Hadoop registration service\nTo install and configure the Hadoop registration service on the edge node, the\nHadoop admin must complete the following tasks:\nDownload the Hadoop registration RPM  to the edge node.\nRun the RPM installer. The rpm is installed in /opt/ibm/dsxhi. If running the\ninstall as the service user, run sudo chown <serviceuser> -R\n/opt/ibm/dsxhi.\nCreate a /opt/ibm/dsxhi/conf/dsxhi_install.conf file using\n/opt/ibm/dsxhi/conf/dsxhi_install.conf.template.HDP as a reference. Edit the\nvalues in the conf file. For guidance, see the inline documentation in the\ndsxhi_install.conf.template.HDP files.\nOptional: If you need to set additional properties to control the location of\nJava, use a shared truststore, or pass additional Java options, create a\n/opt/ibm/dsxhi/conf/dsxhi_env.sh script to export the environment\nvariables:export JAVA=\"/usr/jdk64/jdk1.8.0_112/bin/java\"\nexport JAVA_CACERTS=/etc/pki/java/cacerts\nexport DSXHI_JAVA_OPTS=\"-Djavax.net.ssl.trustStore=$JAVA_CACERTS\" \nIn /opt/ibm/dsxhi/bin, run the ./install.py script to install the Hadoop\nregistration service. The script prompts for inputs on the following options\n(alternatively, you can specify the options as flags): \nAccept the license terms (Hadoop registration uses the same license as\nCloud Pak for Data). You can also accept the license through the\ndsxhi_license_acceptance property in dsxhi_install.conf.\nIf the Ambari URL is specified in dsxhi_install.conf, you will be prompted for\nthe password for the cluster administrator. The value can also be passed\nthrough the --password flag.\nThe master secret for the gateway service. The value can also be passed\nthrough the --dsxhi_gateway_masster_password flag.\nIf the default password for Java cacerts truststore has been changed, the\npassword can be passed through the --\ndsxhi_java_cacerts_password flag.\nThe installation will run pre-checks to validate the prerequisites. If the\ncluster_manager_url is not specified in the dsxhi_install.conf file, then the\npre-checks on the proxyuser settings will not be performed. \nAfter a successful installation, the necessary components (Hadoop registration\ngateway service and Hadoop registration rest service) and optional components\n(Livy for Spark and Livy for Spark 2) will be started. The component logs are\nstored in /var/log/dsxhi, /var/log/livy, and /var/log/livy2. The component PIDs are\nstored in /var/run/dsxhi, /var/run/livy, /var/run/livy2, and\n/opt/ibm/dsxhi/gateway/logs/. \nTo add cacert to the Hadoop registration rest service, go to the /opt/ibm/dsx/util\ndirectory on the edge node and run the add_cert.sh script with the server\naddress, for example, bash add_cert.sh https://master-\n1.ibm.com:443. \n93\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nManage the Hadoop registration service\nPeriodically, the Hadoop admin must manage the Hadoop registration service.\nThese tasks include:\nCheck status of the Hadoop registration service\nIn /opt/ibm/dsxhi/bin, run ./status.py to check the status of Hadoop\nregistration gateway, Hadoop registration rest server, Livy for Spark and\nLivy for Spark services.\nStart the Hadoop registration service\nIn /opt/ibm/dsxhi/bin, run ./start.py to start the Hadoop registration\ngateway, Hadoop registration rest server, Livy for Spark and Livy for Spark\nservices.\nStop the Hadoop registration service\nIn /opt/ibm/dsxhi/bin, run ./stop.py to stop the Hadoop registration\ngateway, Hadoop registration rest server, Livy for Spark and Livy for Spark\nservices.\nAdd certificates for SSL enabled services\nIf the WebHDFS service is SSL enabled, the certificates of the nodemanagers\nand datanodes should to be added to the Hadoop registration gateway trust\nstore. In /opt/ibm/dsxhi/bin/util, run ./add_cert.sh https://host:port\nfor each of the nodemanagers and datanodes to add the certificates to Hadoop\nregistration gateway trust store.Alluxio requirement: To connect to Alluxio using\nremote Spark with Livy, go to Ambari > HDFS > Config > Custom Core-site >\nAdd property in the Ambari web client and fs.alluxio.impl configuration for\nthe remote Spark. See Running Spark on Alluxio  for details. \nManage Cloud Pak for Data for Hadoop registration\nTo maintain control over the access to a Hadoop registration service, a Hadoop\nadmin needs to maintain a list of known Cloud Pak for Data clusters that can\naccess the Hadoop registration service. A Cloud Pak for Data cluster will be\nknown by its URL, which should be passed in when adding to or deleting from\nthe known list. A Hadoop admin can add (or delete) multiple Cloud Pak for Data\nclusters to the known list by passing in a comma separated list of Cloud Pak for\nData cluster. Irrespective of the order in which the arguments for add and\ndelete are specified, the deletes are applied first and then the adds.\nAdd a Cloud Pak for Data cluster to the known list\nIn /opt/ibm/dsxhi/bin, run ./manage_known_dsx.py â-add\n\"url1,url2...urlN\". Once a Cloud Pak for Data cluster is added to\nthe known list, the necessary authentication will be setup and the Cloud\nPak for Data admin can be given a URL to securely connect to the Hadoop\nregistration service.\nDelete a Cloud Pak for Data cluster from the known list\nIn /opt/ibm/dsxhi/bin, run ./manage_known_dsx.py â-delete\n\"url1,url2...urlN\".\nHadoop registration URL for secure access from Cloud Pak for Data\nIn /opt/ibm/dsxhi/bin, run ./manage_known_dsx.py --list to list a table of\nall known Cloud Pak for Data clusters and the associated URL that can be used\nto securely connect from a Cloud Pak for Data cluster to a Hadoop registration\nservice. The Cloud Pak for Data admin can then register the Hadoop cluster.\n94\n-\n-\nUninstall the Hadoop registration service\nTo uninstall the Hadoop registration service and remove the files from\n/opt/ibm/dsxhi, a Hadoop admin can run the ./uninstall.py script in\n/opt/ibm/dsxhi/bin. The uninstallation logs are stored in /var/log/dsxhi,\n/var/log/livy, and /var/log/livy2.\n \n95\n"
      ],
      "document_passages": [
        {
          "passage_text": "HDP only: If Hadoop or Ranger KMS is\nenabled, the service user should <em>have</em> necessary proxyuser privileges in kms-\nsite.xml.\nFor a kerberized cluster: <em>Have</em> the keytab for the service user. This\neliminates the need for every Cloud Pak for Data user to <em>have</em> a valid keytab.",
          "start_offset": 19815,
          "end_offset": 20085,
          "field": "text"
        },
        {
          "passage_text": "Livy for Spark2 Submit jobs to Spark2 on the\nHadoop cluster.\n88\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n In addition, the edge node must meet the following requirements:\n<em>Have</em> Python 2.7 installed.\nCDH only: <em>Have</em> Java Development Kit Version 1.8 installed.\n<em>Have</em> curl 7.19.7-53 or later to allow secure communication between the\nHadoop registration service and Cloud Pak for Data.",
          "start_offset": 19133,
          "end_offset": 19491,
          "field": "text"
        }
      ]
    },
    {
      "document_id": "967cc9c7-8726-48ec-bf1f-c33d8b5082d0",
      "result_metadata": {
        "collection_id": "fbafd5e1-9cf1-c24c-0000-016eeb0a5c9f",
        "document_retrieval_source": "search",
        "confidence": 0.0511335
      },
      "enriched_text": [
        {
          "entities": [
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 58,
                    "begin": 51
                  },
                  "text": "Red Hat"
                },
                {
                  "location": {
                    "end": 483,
                    "begin": 476
                  },
                  "text": "Red Hat"
                },
                {
                  "location": {
                    "end": 874,
                    "begin": 867
                  },
                  "text": "Red Hat"
                },
                {
                  "location": {
                    "end": 1930,
                    "begin": 1923
                  },
                  "text": "Red Hat"
                },
                {
                  "location": {
                    "end": 30450,
                    "begin": 30443
                  },
                  "text": "Red Hat"
                }
              ],
              "text": "Red Hat",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 164,
                    "begin": 161
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 3633,
                    "begin": 3630
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 13205,
                    "begin": 13202
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 26547,
                    "begin": 26544
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 27348,
                    "begin": 27345
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 27742,
                    "begin": 27739
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 27853,
                    "begin": 27850
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 27976,
                    "begin": 27973
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 28153,
                    "begin": 28150
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 28241,
                    "begin": 28238
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 28660,
                    "begin": 28657
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 29590,
                    "begin": 29587
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 29827,
                    "begin": 29824
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 30024,
                    "begin": 30021
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 30146,
                    "begin": 30143
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 30225,
                    "begin": 30222
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 30420,
                    "begin": 30417
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 30654,
                    "begin": 30651
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 31102,
                    "begin": 31099
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 31213,
                    "begin": 31210
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 32011,
                    "begin": 32008
                  },
                  "text": "IBM"
                }
              ],
              "text": "IBM",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 445,
                    "begin": 425
                  },
                  "text": "Red Hat OpenShift.To"
                }
              ],
              "text": "Red Hat OpenShift.To",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 4094,
                    "begin": 4087
                  },
                  "text": "Vnnnbin"
                }
              ],
              "text": "Vnnnbin",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 4494,
                    "begin": 4491
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 4561,
                    "begin": 4558
                  },
                  "text": "ibm"
                }
              ],
              "text": "ibm",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 5209,
                    "begin": 5203
                  },
                  "text": "docker"
                },
                {
                  "location": {
                    "end": 5243,
                    "begin": 5237
                  },
                  "text": "docker"
                },
                {
                  "location": {
                    "end": 5273,
                    "begin": 5267
                  },
                  "text": "docker"
                },
                {
                  "location": {
                    "end": 5315,
                    "begin": 5309
                  },
                  "text": "docker"
                },
                {
                  "location": {
                    "end": 5353,
                    "begin": 5347
                  },
                  "text": "docker"
                },
                {
                  "location": {
                    "end": 5400,
                    "begin": 5394
                  },
                  "text": "docker"
                },
                {
                  "location": {
                    "end": 5445,
                    "begin": 5439
                  },
                  "text": "docker"
                },
                {
                  "location": {
                    "end": 6026,
                    "begin": 6020
                  },
                  "text": "docker"
                },
                {
                  "location": {
                    "end": 6253,
                    "begin": 6247
                  },
                  "text": "docker"
                }
              ],
              "text": "docker",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 7054,
                    "begin": 7051
                  },
                  "text": "nnn"
                }
              ],
              "text": "nnn",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 9760,
                    "begin": 9754
                  },
                  "text": "Docker"
                }
              ],
              "text": "Docker",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 10639,
                    "begin": 10636
                  },
                  "text": "Pak"
                }
              ],
              "text": "Pak",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 11715,
                    "begin": 11709
                  },
                  "text": "TILLER"
                }
              ],
              "text": "TILLER",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 18452,
                    "begin": 18441
                  },
                  "text": "rolesCreate"
                }
              ],
              "text": "rolesCreate",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 18588,
                    "begin": 18581
                  },
                  "text": "steward"
                }
              ],
              "text": "steward",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 19014,
                    "begin": 19004
                  },
                  "text": "dataCreate"
                }
              ],
              "text": "dataCreate",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 28007,
                    "begin": 27998
                  },
                  "text": "GlusterFS"
                }
              ],
              "text": "GlusterFS",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 28048,
                    "begin": 28042
                  },
                  "text": "100 GB"
                }
              ],
              "text": "100 GB",
              "type": "Quantity"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 29315,
                    "begin": 29312
                  },
                  "text": "8GB"
                }
              ],
              "text": "8GB",
              "type": "Quantity"
            }
          ]
        }
      ],
      "metadata": {
        "parent_document_id": "967cc9c7-8726-48ec-bf1f-c33d8b5082d0"
      },
      "extracted_metadata": {
        "sha1": "1BE7B3E33878594216D136CFB1A45428F03B40D6",
        "numPages": "17",
        "filename": "add-ons-integrations-222-238.pdf",
        "file_type": "pdf"
      },
      "text": [
        "1.\n2.\n3.\n1.\n2.\n3.\n Enabling Data Virtualization in Red Hat OpenShift \nYou can use Data Virtualization in an OpenShift environment. \n Before you begin \n \nInstall IBMÂ® Cloud Pak for Data on OpenShift.  About this task \n \nWhen you install Data Virtualization on OpenShift, you can interact with Data\nVirtualization in Cloud Pak for Data, while you use the OpenShift's Kubernetes and\nDocker Registry that is already installed by Red Hat OpenShift.To enable Data\nVirtualization in Red Hat OpenShift, you must complete the following steps:\nConfiguring your Data Virtualization worker node.\nInstalling Data Virtualization on OpenShift.\nProvisioning Data Virtualization on OpenShift.\n \n \n Configuring your Data Virtualization worker node \nData Virtualization requires Security Enhanced Linux (SELinux) to be set to\npermissive mode. However, this requirement differs from the Red Hat OpenShift\n3.11 prerequisites, which require SELinux to be set to enforcing. This\nrequirement might also conflict with your organization's security policies for Linux\nhosts. \n About this task \n \nTo mitigate your security exposure, it is recommended that you select a specific\nnode in your Red Hat OpenShift cluster where you want to install Data Virtualization\nand set SELinux to permissive only on that node. It is recommended that you do\nnot deploy any other services on this node. \nAdditionally, you must label the node as dv-dedicated=selinux so that Data\nVirtualization is deployed on the appropriate node. \n Procedure \n \nRun the following command to get a list of the nodes in your cluster.kubectl get nodes \n \nSelect the worker node where you plan to run Data Virtualization pods.You must\nensure that the selected worker node has sufficient resources to run Data\nVirtualization. For more information on system requirements to run Data\nVirtualization, see Enabling Data Virtualization. If you need to add worker nodes\nto your cluster, see the Red Hat OpenShift product documentation.  \n \nRun the following command to add the dv-dedicated=selinux label to the\nworker node where you plan to run Data Virtualization.kubectl label nodes <node-name> dv-\ndedicated=selinux \n \nFor example, if the selected node name is called kubernetes-foo-node-\n1.c.a-robinson.internal , run the following commandkubectl label nodes\n       kubernetes-foo-node-1.c.a-robinson.internal dv-dedicated=selinux \n \n \n222\n4.\n5.\nA.\nB.\n1.\n2.\n1.\nA.\n-\n-\nB.\nC.\n2.\n-\nRun the following command to verify that the selected node was labeled correctly:\nkubectl get nodes --show-labels \n \nThe selected node should have the dv-dedicated label. To see a complete list\nof labels for the selected node, run the following command.kubectl describe node\nnodename \n Replace nodename with the name of the selected node. For example, if the\nselected node is called kubernetes-foo-node-1.c.a-robinson.internal\n:kubectl describe node kubernetes-foo-node-1.c.a-robinson.internal \n \n \nIn the node that you selected to install Data Virtualization, set\nSELINUX=permissive.\nRun the setenforce 0 command to change SELinux to permissive mode.\nRun the getenforce to validate that SELinux is set to permissive.~]# setenforce 0\n~]# getenforce\nPermissive \n \n What to do next \n \nInstalling Data Virtualization on OpenShift.\nProvisioning Data Virtualization on OpenShift.\n \n Installing Data Virtualization on OpenShift \n Before you begin \n \nEnsure to configure the worker node where you plan to install Data Virtualization. \nProcedure \n \nComplete the following steps to install Data Virtualization on your OpenShift\nenvironment: \nObtain the data virtualization add-on package:\nDownload the appropriate BIN file from IBM Passport Advantage:\nFor Enterprise Edition, download ICP4DATA_DV_x86_Vnnnbin.\nFor Cloud Native Edition, download ICP4DATA_CNE_DV_x86_Vnnn.bin.\n where nnn is the version number of the Data Virtualization add-on. \n \nChange to the directory where the file was downloaded and make the BIN file\nexecutable. For example:chmod +x ICP4DATA_DV_x86_Vnnnbin \nwhere nnn is the version number of the Data Virtualization add-on. \n \nRun the BIN file:./ICP4DATA_DV_x86_Vnnnbin \nwhere nnn is the version number of the Data Virtualization add-on. \nThis file downloads the following TAR file: dv-ppa.tar \n \nSwitch to your Cloud Pak for Data project.oc project namespace \nReplace namespace with the namespace where Cloud Pak for Data is deployed.\nThe default namespace is zen.\n223\n3.\n4.\n5.\n-\nA.\n-\n6.\n \nExtract the contents of the Data Virtualization installation package, the\n/ibm/modules/Data_Virtualization/Packages/dv-ppa.tar file.tar -xvf\n/ibm/modules/Data_Virtualization/Packages/dv-ppa.tar \n \nThe contents of the dv-ppa.tar file are extracted into the /dv-addon subdirectory.\nThe /dv-addon directory has the following file structure:âââ charts\nâ   âââ ibm-dv-addon-nnn.tgz\nâââ images\n   âââ dv-addon-image.tar.gz\n   âââ dv-api-image.tar.gz\n   âââ dv-image.tar.gz\n   âââ dv-init-volume-image.tar.gz\n   âââ dv-mariadb-image.tar.gz\n   âââ dv-service-provider-image.tar.gz\n   âââ dv-unifiedconsole-image.tar.gz\n   âââ dv-unifiedconsole-image.tar.gz\n \nwhere nnn is the version number of the Data Virtualization add-on. \n \nLoad all of the Docker images.docker load < dv-addon-image.tar.gz\ndocker load < dv-api-image.tar.gz\ndocker load < dv-image.tar.gz\ndocker load < dv-init-volume-image.tar.gz\ndocker load < dv-mariadb-image.tar.gz\ndocker load < dv-service-provider-image.tar.gz\ndocker load < dv-unifiedconsole-image.tar.gz\ndocker load < opencontent-common-utils-image.tar.gz  \n \nLog in to the Docker Registry.docker login Docker-registry-URL -u <username> -p <password> \nReplace Docker-registry-URL with the URL of your Docker Registry.\n \nIf the cluster uses a Registry hosted by OpenShift, run the following command:\ndocker login Docker-registry-URL -u $(oc whoami) -p $(oc whoami -t) \nReplace Docker-registry-URL with the URL of your Docker Registry.\n \n Tag your images with the Docker Registry information and with version\ninformationdocker tag dv-addon:nnnDocker-registry-URL/namespace/dv-addon:nnn \ndocker tag dv-singlenode:nnnDocker-registry-URL/namespace/dv-singlenode:nnn \ndocker tag dv-init-volume:nnnDocker-registry-URL/namespace/dv-init-volume:nnn \ndocker tag dv-mariadb:nnnDocker-registry-URL/namespace/dv-mariadb:nnn \ndocker tag dv-unifiedconsole:nnnDocker-registry-URL/namespace/dv-unifiedconsole:nnn \ndocker tag dv-api:nnnDocker-registry-URL/namespace/dv-api:nnn \ndocker tag dv-service-provider:nnnDocker-registry-URL/namespace/dv-service-provider:nnn \ndocker tag opencontent-common-utils:image-versionDocker-registry-URL/namespace/opencontent-common-utils:\nimage-version \nwhere nnn is the version number of the Data Virtualization add-on. \n224\n-\n-\n-\n7.\n-\n-\n-\n8.\n-\n-\n-\n1.\nReplace Docker-registry-URL with the URL of your Docker Registry.\nReplace namespace with the namespace where Cloud Pak for Data is deployed.\nThe default namespace is zen.\nReplace image-version with the version of the opencontent-common-utilsDocker\nimage.\n \nPush the images to the Docker Registry.docker push Docker-registry-URL/namespace/dv-addon:\nnnn \n   docker push Docker-registry-URL/namespace/dv-singlenode:nnn \n   docker push Docker-registry-URL/namespace/dv-init-volume:nnn \n   docker push Docker-registry-URL/namespace/dv-mariadb:nnn \n   docker push Docker-registry-URL/namespace/dv-unifiedconsole:nnn \n   docker push Docker-registry-URL/namespace/dv-api:nnn \n   docker push Docker-registry-URL/namespace/dv-service-provider:nnn \n   docker push Docker-registry-URL/namespace/opencontent-common-utils:image-version \nwhere nnn is the version number of the Data Virtualization add-on. \nReplace Docker-registry-URL with the URL of your Docker Registry.\nReplace namespace with the namespace where Cloud Pak for Data is deployed.\nThe default namespace is zen.\nReplace image-version with the version of the opencontent-common-utilsDocker\nimage.\n \nGo to the charts directory and install the Data Virtualization add-on.cd charts\nhelm install ./dv-addon-nnn.tgz --set docker_registry_prefix=Docker-registry-URL/namespace--tls \nwhere nnn is the version number of the Data Virtualization add-on. \nReplace Docker-registry-URL with the URL of your Docker Registry.\nReplace namespace with the namespace where Cloud Pak for Data is deployed.\nThe default namespace is zen.\nThe --tls parameter is optional. Use this parameter if TLS is enabled for Helm.\n \n \n Provisioning Data Virtualization on OpenShift \n Before you begin \n \nEnsure to configure the worker node where you plan to install Data Virtualization. \nAbout this task \n \nBefore you use Data Virtualization, you must deploy and provision the add-on to\nyour Cloud Pak for Data in the OpenShift environment. Procedure \n \nTo provision the Data Virtualization add-on: \nCreate a YAML file called icpdata-configmap.yaml with the following contents:\napiVersion: v1\ndata:\n docker_registry_prefix: Docker-registry-URL/namespace \n docker_registry_secret: Docker-registry-secret \nkind: ConfigMap\nmetadata:\n225\n-\n-\n-\n2.\n3.\n4.\nA.\nB.\n labels:\n   icpdata-configmap: \"true\"\n name: icpdata-configmap\n namespace: namespace \nReplace the following values:\nReplace the value of the docker_registry_prefix parameter with the URL of the\nDocker Registry where the Docker images are loaded. This is the address that\nyour node uses to access the Docker Registry. \nIf you have a Docker pull secret, replace the value of the docker_registry_secret\nparameter with your Docker pull secret. The Docker pull secret is used to pull\nimages from the Docker Registry. Note: If you are not using OpenShiftâs\nintegrated Docker Registry, ensure that the pod's service account of the\nOpenShift project is configured with yourDocker pull secret. For more\ninformation, see Image Pull Secrets in the OpenShift documentation. \nIf you don't have a Docker pull secret, you can ignore the docker_registry_secret\nparameter. \nReplace namespace with the namespace where Cloud Pak for Data is deployed.\nThe default namespace is zen.\n \n \nCreate the icpdata-configmap.yaml file in your project directory.oc create -f icpdata-\nconfigmap.yaml \n \nA configuration map file is created in your project. \nGet a list of the storage classes that are available to provision Data Virtualization\nin the OpenShift cluster.oc get storageclasses\nNAME                PROVISIONER               AGE\nglusterfs-storage   kubernetes.io/glusterfs   7d \nYou will need the name of the storage class to provision Data Virtualization. \n \nIf Helm on your OpenShift environment is set up to use TLS, ensure that the\nzen-core-api binds the helm-secret in your namespace.Note: The following\nset of instructions uses the default helm_secret that comes out of the Cloud\nPak for Data installation. You can use the oc create secret command to\ncreate a Helm secret. \n \nEdit the zen-core-api deployment.oc edit deploy\n     zen-core-api \n \nEnsure that the volumeMounts section in the zen-core-api deployment has\nthe following content:volumeMounts:\n       - mountPath: /var/run/sharedsecrets\n         name: zen-service-broker-secret\n       - mountPath: /user-home\n         name: user-home-mount\n       - mountPath: /certs\n         name: helm-secret\n         readOnly: true \n226\nC.\n5.\n6.\n \nEnsure that the volumes section in the zen-core-api deployment has the\nfollowing content:volumes:\n     - name: zen-service-broker-secret\n       secret:\n         defaultMode: 420\n         secretName: zen-service-broker-secret\n     - name: user-home-mount\n       persistentVolumeClaim:\n         claimName: user-home-pvc\n     - name: helm-secret\n       secret:\n         defaultMode: 420\n         secretName: helm-secret \n \nIf Helm on your OpenShift environment is not set up to use TLS, specify the\nenvironment variables in the zen-core-api.- env:\n       - name: TILLER_NAMESPACE\n         value: kube-system\n       - name: TILLER_PORT\n         value: \"44134\"\n       - name: NO_TLS\n         value: \"true\" \n \nCreate the required security context constraint with the following settings:\nallowHostDirVolumePlugin: true \nallowHostIPC: true\nallowHostNetwork: false\nallowHostPID: false\nallowHostPorts: false\nallowPrivilegedContainer: false \nallowedCapabilities:\n- IPC_OWNER\n \n- SYS_NICE\nallowedFlexVolumes: null\napiVersion: security.openshift.io/v1\ndefaultAddCapabilities: null\nfsGroup:\ntype: RunAsAny\ngroups:\n- cluster-admins\n \nkind: SecurityContextConstraints\nmetadata:\nannotations:\n  kubernetes.io/description: zenuid provides all features of the restricted SCC\n    but allows users to run with any UID and any GID.\n227\n-\n-\n7.\ncreationTimestamp: 2019-02-28T03:05:36Z\nname: zenuid\n \npriority: 10\nreadOnlyRootFilesystem: false\nrequiredDropCapabilities: null\nrunAsUser:\ntype: RunAsAny\nseLinuxContext:\n type: RunAsAny \nsupplementalGroups:\ntype: RunAsAny\nusers:\n-  system:serviceaccount:namespace:service-account-name \nvolumes:\n- configMap\n- downwardAPI\n- emptyDir\n - hostPath \n- persistentVolumeClaim\n- projected\n- secret \nReplace namespace with the namespace where Cloud Pak for Data is deployed.\nThe default namespace is zen.\nReplace service-account-name with a service account associated with the\nCloud Pak for Data namespace that has the authority to deploy. \n \nProvision Data Virtualization.\n \n228\n-\n-\n-\n-\n-\n-\n-\n-\n Giving users access to Data Virtualization \nIBMÂ® Cloud Pak for Data users that need to use Data Virtualization functions must\nbe assigned specific Data Virtualization roles based on their job description. \n About this task \n \nAccess to Data Virtualization is administered by Cloud Pak for Data user\nmanagement functions. Access within Data Virtualization is managed by the Db2Â®\ndatabase manager.\nAuthentication\nIf you authenticate to Data Virtualization directly by using JDBC client\napplications, you must specify the Data Virtualization username. The Data\nVirtualization username is composed of the Cloud Pak for Data user ID of a\ncorresponding Cloud Pak for Data username.A Data Virtualization Admin must\nexplicitly add Cloud Pak for Data users to the Data Virtualization service in\norder for these users to authenticate to the service directly. When the Data\nVirtualization Admin adds a Cloud Pak for Data user to the Data Virtualization\nservice, the Data Virtualization username and password are automatically\ngenerated for the user. The Data Virtualization username has the following\nformat: user{id}, where {id} corresponds to the Cloud Pak for Data user ID. \nAuthorization\nCloud Pak for Data users are first authorized at a service-level to access the\nData Virtualization service by using the corresponding Data Virtualization\nusername. Data Virtualization roles are used for authorization, independently of\ngroup membership. Data Virtualization uses role-based access control for\ndatabase-level and object-level authorization. Data Virtualization follows\nauthorization based on the Db2 Authorities and Privilege model. Various\nadministrative authorities exist at the Data Virtualization instance level and at\nthe database level. \n \nThere are four user roles defined for Data Virtualization. These roles are specific to\nthe Data Virtualization environment and are granted to existing Cloud Pak for Data\nusers. In order for a user to have access to the Data Virtualization service, you must\nassign them one of the following Data Virtualization roles:\nData Virtualization Admin\nThe user who provisions the Data Virtualization add-on is automatically\nassigned the Data Virtualization Admin role. After the add-on has been\nprovisioned, the Data Virtualization Admin can give other users access to the\nservice.The Data Virtualization Admin is considered to be the manager of the\nData Virtualization instance and assigns appropriate Data Virtualization roles to\nCloud Pak for Data users. \nData Virtualization Engineer\nConfigures the data sources, virtualizes data, and manages access to virtual\nobjects. Users with this role can create a virtual table or view and grant access\nto it to users with any Data Virtualization role. By default, every virtual object\nthat is created in Data Virtualization is private. This means that in order for a\nvirtual object to be accessed by a user other than its creator, access to the\nvirtual object must be granted.Data source administrators are expected to\nprovide access to a Data Virtualization Engineer to virtualize data. Users with\n229\n-\n-\n-\n-\nthis role service and fulfill data requests from Data Virtualization users.  \nData Virtualization User\nData Virtualization users can request access to virtualized data or data in\ngeneral by initiating a data request. Users with this role can create views of\nvirtual tables to which they have access. \nData Virtualization Steward\nData Virtualization Stewards can approve requests to publish virtual tables or\nviews to the enterprise data catalog. \n \nThe following table summarizes the Data Virtualization menu functions that each of\nthe Data Virtualization user roles is able to access. \nImportant: If users submit requests to publish virtual tables or views to the enterprise\ndata catalog, you must give at least one Cloud Pak for Data user with data steward\nrole access to the Data Virtualization service as a steward. This user can thus\napprove publish requests.  \nPermissions of Data Virtualization rolesThe following table describes the\npermissions that are associated with each Data Virtualization role.\nData\nvirtualization\nfeatures\nAdmin Engineer User Steward\nProvision\nData\nVirtualizatio\nn\nâ\nManage\nusers\nâ\nData\nsources\nâ â\nVirtualize â â\nMy data â â â â\nConnection\ndetails\nâ â â â\nService\ndetails\nâ â â â\nSQL editor â â â â\nRoles Permissions\n230\nImportant: To enable complete access control, including privileges to grant\npermissions to other users, and to remove a virtual object, Data Virtualization\nEngineers must be granted the access control administration authority\n(ACCESSCTRL) at the database-level. The access control administration authority\ncan be granted the GRANT CONTROL statement:GRANT CONTROL on <object> to ROLE\nDV_ENGINEER \n For more information about the access control administration authority, see the Db2\nproduct documentation. \n \n \n \n Assigning user roles \n Before you begin \n \nYou can use LDAP to authorize your users before adding a Data Virtualization role.\nSee Connecting to your LDAP server for details. About this task \n \nTo assign a Data Virtualization role to an existing Cloud Pak for Data user, the Data\nData Virtualization Admin Administrate the service\nAdministrate the\ndatabaseAccess dataManage\ndata sourcesManage users and\nassign Data Virtualization\nrolesCreate and share any\nschemaApprove publish\nrequests to the enterprise data\ncatalog. Data Virtualization\nAdmins must also have the data\nsteward role in Cloud Pak for\nData to approve publish\nrequests to the data catalog.\nData Virtualization Engineer Access connection\ndetailsManage data\nsourcesCreate virtual tables and\nviewsCreate and manage private\nschema\nData Virtualization User Access connection detailsCreate\nvirtual views over existing virtual\ntables and viewsCreate and\nmanage private schema\nData Virtualization Steward Access connection\ndetailsAccess dataCreate virtual\nviews over existing virtual tables\nand viewsCreate and manage\nprivate schemaApprove publish\nrequests to the enterprise data\ncatalog. Data Virtualization\nStewards must also have the\ndata steward role in Cloud Pak\nfor Data to approve publish\nrequests to the data catalog.\n231\n1.\n2.\n3.\n4.\n5.\n6.\n1.\n2.\n3.\nA.\nB.\nC.\n4.\nA.\nVirtualization Admin must complete the following steps: \n Procedure \n \nGo to Collect > Virtualized data. Currently configured data sources are displayed.\nClick Menu > Manage users > Users to see a list of users that have been\nassigned a Data Virtualization role.\nClick Add users to assign Data Virtualization roles to Cloud Pak for Data users.\nSelect the check box for the username that you want to assign a Data\nVirtualization role.\nClick the drop-down arrow for the user and select the Data Virtualization role from\nthe drop-down list.\nClick Add.\n \n Modifying user roles \n About this task \n \nTo modify a role for an existing Data Virtualization user, the Data Virtualization\nAdmin must complete the following steps: \n Procedure \n \nGo to Collect > Virtualized data.\nClick Menu in the upper right corner of the page and select Manage users.\nTo change a user's role, in Data Virtualization: \nHover over the username and click the action menu. \nClick Edit role.\nClick the drop-down arrow for the appropriate username and select the user\nrole from the drop-down list.\nTo remove a user's access to Data Virtualization functions:\nHover over the username and click the action menu. Click Remove to remove\nthe user's access to Data Virtualization.\n \n232\n-\n-\n-\n-\n1.\n2.\n3.\n4.\nA.\nB.\n5.\nA.\nB.\n6.\nA.\nB.\n7.\n8.\nA.\nB.\nC.\n-\n-\n Managing access to virtual objects \nData Virtualization administrators can grant user access to virtual objects in Data\nVirtualization. \n About this task \n \nData Virtualization administrators have access to all virtual objects in Data\nVirtualization. Data Virtualization stewards have access to all virtual objects that are\ncreated.  Users assigned to the Data Virtualization Admin and Engineer roles can\ngrant access to specific virtual objects for other users in Data Virtualization. To use\nvirtual objects, you can:\nManage access to virtual objects\nRevoking access to virtual objects for user roles\nManaging visibility of virtual objects\nCreating schemas\n \n Procedure \n \nTo manage user access to virtual objects, complete the following steps: \nFrom the Data Virtualization page, go to Menu > My data. You can see a list of\nvirtual objects that you can access. \nHover over the virtual object name and click the action menu.\nClick Manage access.\nTo grant access to specific users, click Grant access > To specific users:\nSelect the check box for the username that you want to grant access to the\nvirtual object.\nClick Add.\nTo grant access to user roles, click Grant access > To Data Virtualization roles:\nSelect the check box for the Data Virtualization role that you want to grant\naccess to the virtual object.\nClick Add.\nTo remove a user's access to virtual objects in Data Virtualization:\nSelect the check box for the username that you want to remove access to the\nvirtual object.\nClick Remove to remove the user's access to Data Virtualization.\nTo grant all Data Virtualization users access to a virtual object, disable the\nRestrict table access option.\nTo view a list of permissions that were explicitly granted to, or inherited by the\nuser.\nClick Menu > Manage users.\nHover over the username and click the action menu.\nClick Manage access\nIn the Granted permissions tab, you can see virtual objects to which the user\nhas been explicitly granted access permissions.\nIn the Inherited permissions tab, you can see virtual objects that the user can\naccess via inherited permissions. Permissions can be inherited from user\nroles, or from virtual objects that are publicly available to all users.\n \n233\n1.\n2.\n3.\n4.\n1.\n2.\n-\n-\n \n Revoking access to virtual objects for user roles \n About this task \n \nTo modify access of a user role to a virtual object, complete the following steps: \nProcedure \n \nClick Menu > Manage users > Roles to see the list of roles in Data Virtualization.\nYou can click the drop-down arrow of a user role to see a list of users that have\nbeen assigned a Data Virtualization role. \nHover over the user role, click the action menu, and click View and revoke object\nprivileges. \nSelect the check box for the virtual object to which you want to revoke access.\nClick Revoke to remove access of users with this role to the virtual object.\n \n Managing visibility of virtual objects \nYou can determine whether Data Virtualization users can view virtual objects that\nthey cannot access. \n About this task \n \nBy default, Data Virtualization users can see column and table names only of virtual\nobjects that they can access. For example, if a user is not allowed to perform a\nSELECT statement on a table, the table is not be listed when you perform the LIST\nTABLES operation. Procedure \n \nTo manage visibility of virtual objects: \nClick Menu > Service details.\nGo to the Virtual object access control section:\nEnable the Restrict visibility of virtual objects list to authorized users option to\nprevent users from seeing column and table names of virtual objects that they\ncannot access. This option is enabled by default.\nDisable the Restrict visibility of virtual objects list to authorized users option to\nallow users to see column and table names of virtual objects that they cannot\naccess.\n \n Creating schemas \nYou can use schemas to group virtual objects. A virtual object can belong to only\none schema. Use the CREATE SCHEMA statement to create schemas.  \n About this task \n \nIf a schema does not exist in your Data Virtualization add-on, you must create a\nschema explicitly. Any user can create a schema with their own authorization name\nby using a CREATE SCHEMA statement. For example, user with username\nUSER1001 can create a private schema named USER1001. To enable other users\nor roles to use the schema, USER1001 must grant CREATEIN, ALTERIN, or\nDROPIN schema permissions to these users or roles. Additionally, granting these\npermissions to PUBLIC enables the schema to be shared by all users. \nData Virtualization Admins can create schemas based on names other than the\n234\n-\n-\n-\n-\nexisting userâs authorization name. Data Virtualization Admins must grant\nappropriate permissions to other users or roles for them to use the schema to create\nvirtual objects. \nAs Data Virtualization Admin, you can grant IMPLICIT_SCHEMA permissions to a\nuser with the Data Virtualization Engineer role for each project. Data Virtualization\nEngineers can then create schemas to be used by other users in their project. When\nschemas are created by using the IMPLICIT_SCHEMA permission, note that\nPUBLIC is granted the CREATIN permission on these schemas. \n Procedure \n \nTo create a schema while creating a virtualized table:\nIf you have the Data Virtualization Engineer or User roles, leave the Schemas\nfield as default to create a schema with your user ID.\nIf you have the Data Virtualization Admin role, enter the new schema name in the\nSchemas field.\nFor more information on how to create virtualized tables, see Virtualizing data. \nTo create a schema from the command line or SQL Editor, enter the following\nstatement: CREATE SCHEMA schema-name \nFor more information about creating schemas, see Creating schemas in the Db2Â®\nproduct documentation. \n \n \n235\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n Creating an integrated database \nYou can optionally create one or more databases within IBMÂ® Cloud Pak for Data.\nAfter you create an integrated database, you can create a connection to the\ndatabase so that the contents of the database can be discovered, governed, and\nused for analytics. \n Integrated databases can be used to:\nOffload some of your data workload to the system where the governance and\nanalysis occurs\nQuickly provision databases if you don't have existing databases\nCreate a sandbox environment\n \nYou can create several different types of integrated databases in Cloud Pak for Data\n. \nImportant: The required resources for each database are the minimum\nrequirements. You should size your database based on your expected workload.\nDetailed requirements and use cases for integrated databases provides more in-\ndepth information on planning for an integrated database. \n \n \n IBM Db2 Advanced Enterprise Server Edition \nRelational database that delivers advanced data management and analytics\ncapabilities for transactional workloads.  \nNodes:\n1 node for standard deployments\n3 nodes for HADR deployments\nSupported storage:\nhostPath storage (not supported on HADR deployments)\nNFS storage\nCPU and memory:Varies based on your expected workload \nFor more information, see IBM Db2 Advanced Enterprise Server Edition in Detailed\nrequirements and use cases for integrated databases.  \n IBM Db2 Event Store \nA data store capable of high speed ingest and deep, real-time analytics. \nNodes: 3\nSupported storage:\nIBM Cloud Object Storage\nGlusterFS storage\nLocal storage (per node): 100 GB SSD with XFS format\nCPU and memory:Varies based on your expected workload \nFor more information, see IBM Db2 Event Store in Detailed requirements and use\ncases for integrated databases.  \n IBM Db2 Warehouse \nA high-performing analytic engine that combines in-memory processing with\nintegrated database analytics. Runs on either a single node, for cost-efficiency, or\non multiple nodes for better performance. \nNodes: 1, or 2-999\nSupported storage:\nhostPath storage\n236\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n1.\n2.\n3.\n4.\n-\n-\n-\n-\n-\nNFS storage\nCPU and memory:Varies based on your expected workload \nFor more information, see IBM Db2 Warehouse in Detailed requirements and use\ncases for integrated databases. \n MongoDB \nMongoDB is a flexible and scalable database that is designed to store, query, and\nindex documents. \nNodes: 1\nSupported storage:\nLocal storage\nNFS storage\nCPU and memory:Varies based on your expected workload \nFor more information, see MongoDB in Detailed requirements and use cases for\nintegrated databases. \n PostgreSQL \nPostgreSQL is a popular, open source SQL database that is extensible, ACID\ncompliant, and supports high availability and backup and restore functions. \nNodes: 2 (minimum)\nSupported storage:\nNFS storage\nLocal storage\nCPU and memory:1 core, 8GB (minimum) \nFor more information, see PostgreSQL in Detailed requirements and use cases for\nintegrated databases. \n Creating a database \nTo create an integrated database, you must complete the following tasks:\nDownloading the database installation package.\nPreparing your IBM Cloud Private cluster for a database.\nConfiguring database storage.\nCreating a database deployment on the cluster.\n \nDetailed requirements and use cases for integrated databases \n If you plan to create an integrated database in your IBM Cloud Pak for Data\ncluster, ensure that you have sufficient resources for your planned workload. \nDownloading the database installation package \n The software for installing a database in your IBM Cloud Pak for Data cluster is\nprovided as a separate package, which you must download to your cluster.\nPreparing your IBM Cloud Private cluster for a database \n If you plan to create a database in IBM Cloud Pak for Data, you must prepare your\ncluster so that you can successfully provision the database.\nPreparing your OpenShift cluster for the database \n If you plan to create a database in IBM Cloud Pak for Data on Red Hat OpenShift,\nyou must prepare your cluster so that you can successfully provision the database.\nEnabling GPFS support for Db2 Warehouse databases \n As an optional storage file system, you can set up an IBM SpectrumÂ® Scale\nstorage cluster to act as a persistent volume for stateful Db2Â® Warehouse\ndatabases.\n237\n-\n-\n-\n-\n-\nConfiguring database storage \n You must use the appropriate type of storage for your integrated database. You\ncan configure the storage before you create the database deployment, or when\nyou create the database deployment.\nCreating a database deployment on the cluster \n You create a database deployment on your cluster from the IBM Cloud Pak for\nData web client.\nUpdating the Db2 password secrets \n The Db2 administration accounts on your IBM Cloud Pak for Data cluster are\nprotected by password secrets. During deployment of Db2 Warehouse, these\npassword secrets are automatically generated and securely stored for these\naccounts. These accounts are used by Db2 Warehouse to handle administrative\ntasks on the database. If you need to change them to comply with specific\npassword regulations, or if your security situation changes, you can use this\nmethod to update the password secrets at any point in time.\nGiving users access to the database \n Whether you need to give users access to the database depends on the type of\ndatabase that you deployed.\nConfiguring SSL client connections with Db2 \n Use secure sockets layer (SSL) to create secure connections from Db2 clients to\nthe integrated  Db2 Warehouse database server deployed on IBM Cloud Pak for\nData.\nParent topic:Installing data source add-ons \n \n238\n"
      ],
      "document_passages": [
        {
          "passage_text": ".\n-\n-\n Managing access to virtual objects \nData Virtualization administrators can grant user access to virtual objects in Data\nVirtualization. \n About this task \n \nData Virtualization administrators <em>have</em> access to all virtual objects in Data\nVirtualization. Data Virtualization stewards <em>have</em> access to all virtual objects that are\ncreated.",
          "start_offset": 20643,
          "end_offset": 20982,
          "field": "text"
        },
        {
          "passage_text": "Procedure \n \nTo create a schema while creating a virtualized table:\nIf you <em>have</em> the Data Virtualization Engineer or User roles, leave the Schemas\nfield as default to create a schema with your user ID.\nIf you <em>have</em> the Data Virtualization Admin role, enter the new schema name in the\nSchemas field.\nFor more information on how to create virtualized tables, see Virtualizing data.",
          "start_offset": 25814,
          "end_offset": 26191,
          "field": "text"
        }
      ]
    },
    {
      "document_id": "3a4df9e7-9edd-4c8e-8dd9-7effbe49ca7e",
      "result_metadata": {
        "collection_id": "fbafd5e1-9cf1-c24c-0000-016eeb0a5c9f",
        "document_retrieval_source": "search",
        "confidence": 0.05101
      },
      "enriched_text": [
        {
          "entities": [
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 112,
                    "begin": 109
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 406,
                    "begin": 403
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 3024,
                    "begin": 3021
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 3815,
                    "begin": 3812
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 5983,
                    "begin": 5980
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 6121,
                    "begin": 6118
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 6206,
                    "begin": 6203
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 6245,
                    "begin": 6242
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 6309,
                    "begin": 6306
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 6429,
                    "begin": 6426
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 6573,
                    "begin": 6570
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 6636,
                    "begin": 6633
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 6704,
                    "begin": 6701
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 6902,
                    "begin": 6899
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 6949,
                    "begin": 6946
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 6990,
                    "begin": 6987
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 7104,
                    "begin": 7101
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 7312,
                    "begin": 7309
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 7399,
                    "begin": 7396
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 7482,
                    "begin": 7479
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 7547,
                    "begin": 7544
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 7598,
                    "begin": 7595
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 7877,
                    "begin": 7874
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 7963,
                    "begin": 7960
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 8090,
                    "begin": 8087
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 8277,
                    "begin": 8274
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 8448,
                    "begin": 8445
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 8551,
                    "begin": 8548
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 8643,
                    "begin": 8640
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 9062,
                    "begin": 9059
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 10207,
                    "begin": 10204
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 11137,
                    "begin": 11134
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 11263,
                    "begin": 11260
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 12459,
                    "begin": 12456
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 12947,
                    "begin": 12944
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 13984,
                    "begin": 13981
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 14131,
                    "begin": 14128
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 14243,
                    "begin": 14240
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 14459,
                    "begin": 14456
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 14718,
                    "begin": 14715
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 14908,
                    "begin": 14905
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 15388,
                    "begin": 15385
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 15446,
                    "begin": 15443
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 15509,
                    "begin": 15506
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 15914,
                    "begin": 15911
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 16127,
                    "begin": 16124
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 16537,
                    "begin": 16534
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 16585,
                    "begin": 16582
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 16758,
                    "begin": 16755
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 17117,
                    "begin": 17114
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 17294,
                    "begin": 17291
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 18031,
                    "begin": 18028
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 18441,
                    "begin": 18438
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 18489,
                    "begin": 18486
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 18750,
                    "begin": 18747
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 18783,
                    "begin": 18780
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 18938,
                    "begin": 18935
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 20523,
                    "begin": 20520
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 20635,
                    "begin": 20632
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 21296,
                    "begin": 21293
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 22527,
                    "begin": 22524
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 26863,
                    "begin": 26860
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 28425,
                    "begin": 28422
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 29325,
                    "begin": 29322
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 29439,
                    "begin": 29436
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 30303,
                    "begin": 30300
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 30790,
                    "begin": 30787
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 30932,
                    "begin": 30929
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 30984,
                    "begin": 30981
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 31099,
                    "begin": 31096
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 31794,
                    "begin": 31791
                  },
                  "text": "IBM"
                }
              ],
              "text": "IBM",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 143,
                    "begin": 136
                  },
                  "text": "Red Hat"
                }
              ],
              "text": "Red Hat",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 1139,
                    "begin": 1136
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 3853,
                    "begin": 3850
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 3980,
                    "begin": 3977
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 8924,
                    "begin": 8921
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 10746,
                    "begin": 10743
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 11392,
                    "begin": 11389
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 34786,
                    "begin": 34783
                  },
                  "text": "ibm"
                }
              ],
              "text": "ibm",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 1525,
                    "begin": 1508
                  },
                  "text": "Red Hat OpenShift"
                }
              ],
              "text": "Red Hat OpenShift",
              "type": "Organization"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 2484,
                    "begin": 2477
                  },
                  "text": "@MASTER"
                },
                {
                  "location": {
                    "end": 32779,
                    "begin": 32772
                  },
                  "text": "@MASTER"
                }
              ],
              "text": "@MASTER",
              "type": "TwitterHandle"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 3236,
                    "begin": 3234
                  },
                  "text": "$1"
                },
                {
                  "location": {
                    "end": 4350,
                    "begin": 4348
                  },
                  "text": "$1"
                }
              ],
              "text": "$1",
              "type": "Quantity"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 9510,
                    "begin": 9503
                  },
                  "text": "5.0.2.0"
                }
              ],
              "text": "5.0.2.0",
              "type": "IPAddress"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 11650,
                    "begin": 11639
                  },
                  "text": "IBM Storage"
                }
              ],
              "text": "IBM Storage",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 12239,
                    "begin": 12237
                  },
                  "text": "va"
                }
              ],
              "text": "va",
              "type": "Location"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 16269,
                    "begin": 16257
                  },
                  "text": "deployed.For"
                },
                {
                  "location": {
                    "end": 18173,
                    "begin": 18161
                  },
                  "text": "deployed.For"
                }
              ],
              "text": "deployed.For",
              "type": "PrintMedia"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 17789,
                    "begin": 17778
                  },
                  "text": "mount point"
                },
                {
                  "location": {
                    "end": 19352,
                    "begin": 19341
                  },
                  "text": "mount point"
                }
              ],
              "text": "mount point",
              "type": "GeographicFeature"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 19660,
                    "begin": 19647
                  },
                  "text": "mount command"
                }
              ],
              "text": "mount command",
              "type": "Organization"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 21836,
                    "begin": 21830
                  },
                  "text": "512 GB"
                },
                {
                  "location": {
                    "end": 21876,
                    "begin": 21870
                  },
                  "text": "512 GB"
                }
              ],
              "text": "512 GB",
              "type": "Quantity"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 23708,
                    "begin": 23683
                  },
                  "text": "Db2 Warehouse and MongoDB"
                }
              ],
              "text": "Db2 Warehouse and MongoDB",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 24897,
                    "begin": 24893
                  },
                  "text": "1 TB"
                },
                {
                  "location": {
                    "end": 25397,
                    "begin": 25393
                  },
                  "text": "1 TB"
                },
                {
                  "location": {
                    "end": 26480,
                    "begin": 26476
                  },
                  "text": "1 TB"
                },
                {
                  "location": {
                    "end": 27708,
                    "begin": 27704
                  },
                  "text": "1 TB"
                }
              ],
              "text": "1 TB",
              "type": "Quantity"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 24946,
                    "begin": 24939
                  },
                  "text": "1000 GB"
                }
              ],
              "text": "1000 GB",
              "type": "Quantity"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 26180,
                    "begin": 26171
                  },
                  "text": "GlusterFS"
                }
              ],
              "text": "GlusterFS",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 31241,
                    "begin": 31234
                  },
                  "text": "MongoDB"
                },
                {
                  "location": {
                    "end": 31491,
                    "begin": 31484
                  },
                  "text": "MongoDB"
                }
              ],
              "text": "MongoDB",
              "type": "Company"
            }
          ]
        }
      ],
      "metadata": {
        "parent_document_id": "3a4df9e7-9edd-4c8e-8dd9-7effbe49ca7e"
      },
      "extracted_metadata": {
        "sha1": "D1CE38BB98A0A2739788D7A25790E398AF9053DC",
        "numPages": "21",
        "filename": "add-ons-integrations-255-275.pdf",
        "file_type": "pdf"
      },
      "text": [
        "1.\n2.\n3.\n4.\n-\n-\n-\n1.\n Preparing your OpenShift cluster for the database \nIf you plan to create a database in IBMÂ® Cloud Pak for Data on Red Hat OpenShift,\nyou must prepare your cluster so that you can successfully provision the database. \n Before you begin \n \nThese instructions are intended to prepare a Red Hat OpenShift cluster. If you are\nnot preparing an OpenShift cluster, refer to Preparing your IBM Cloud Private cluster\nfor a database. \nEnsure that you completed the steps in Downloading the database installation\npackage. \nImportant: This task assumes that you already have nodes in your cluster that meet\nthe minimum specifications for the database. If you need to add more nodes to your\ncluster to meet the number of required nodes, contact your OpenShift cluster\nadministrator to add the necessary nodes.Use the information in Detailed\nrequirements and use cases for integrated databases to determine the minimum\nspecifications, based on the type of database you are deploying.  \n \nAs a prerequisite, Cloud Pak for Data checks that you meet all system requirements.\n \nIf Cloud Pak for Data is not installed in the default /ibm/ installation directory, you\nmust specify the --install-directory\nINSTALL-DIRECTORY parameter and replace INSTALL-DIRECTORY with the path\nof the Cloud Pak for Data installation directory. \nDb2Â® Warehouse requires Security Enhanced Linux (SELinux) to be set to\npermissive or disabled mode on the nodes that run Db2 Warehouse. However, this\nrequirement differs from the Red Hat OpenShift 3.11 prerequisites, which require\nSELinux to be set to enforcing. This requirement might also conflict with your\norganization's security policies for Linux hosts. To set SELinux, follow these steps: \nOpen the SELinux configuration file to edit:$ vi /etc/selinux/config \nChange the line SELINUX=enforcing to SELINUX=permissive or\nSELINUX=disabled.\nSave the changes and close the file. Reboot the node to allow the SELinux\nchanges to take effect:$ reboot \nWhen the node starts up again, confirm that SELinux is in permissive or disabled\nmode:$ getenforce\nPermissive \n About this task \n \nCloud Pak for Data includes scripts that you can run to prepare your cluster. Use the\nscript to complete the following tasks:\nLabel nodes that can be used for a database\nEnable the database package\nConfigure an NFS server for storing your data\n Procedure \n \nTo prepare your cluster: \nSSH into the master node (master-1) on your Cloud Pak for Data cluster:ssh\naccount@MASTER-1-IP \n255\n2.\n-\n-\n-\n-\n-\n3.\n-\n-\n-\n-\n4.\n-\n-\n-\n-\nReplace account with the user account that has permission to run oc commands. \nLabel the database nodes in your cluster:You must label the dedicated nodes in\nyour cluster to be used by the database.oc label node NODE-NAME icp4data=database-dbtype \nReplace the NODE-NAME with the name of the node that is to be labeled.\nReplace dbtype with the kind of database that you are using on the node:\ndb2wh - for Db2 Warehouse\ndb2oltp - for Db2 Advanced Enterprise Server Edition\ndb2eventstore - for IBM Db2 Event Store\nmongodb - for MongoDB\npostgresql - for PostgreSQL\n \n \nEnable the database installation package on Cloud Pak for Data.oc exec -it $(oc get\npods -n=project | grep zen-database-core | awk {'print $1'}) \\\n-n project -- /tools/configure-database.sh --master MASTER-1-IP \\\n--ppa-archive ARCHIVE_FILE_NAME --icp-user $(oc whoami) --icp-password $(oc whoami -t) \\\n--registry docker-registry.default.svc:5000  --tls-verify false  --tiller-namespace tiller \\\n--helm-bin-path \"/root/linux-amd64\" --disable-tls --namespace project --enable \nReplace project with the OpenShift namespace where Cloud Pak for Data is\ndeployed. \nReplace MASTER-1-IP with the IP address of the master-1 node.\nReplace ARCHIVE-FILE-NAME with the fully qualified path of the package that\nyou downloaded from IBM Passport Advantage. For example:\n/ibm/InstallPackage/databases/db2wh-icp4d_x86_64_2019-07-17_12-40.tar.gz\nIf Cloud Pak for Data is not installed in the default /ibm/ directory, specify the --\ninstall-directory\nINSTALL-DIRECTORY parameter and replace INSTALL-DIRECTORY with the\nCloud Pak for Data installation directory.\n \nIf you plan to use NFS storage for your database, you can set up a server as an\nNFS server by running the following command:kubectl exec -it $(kubectl get pods -n\nproject | grep zen-database-core | awk {'print $1'}) \\\n-n project -- /tools/configure-database.sh --password SSH-PASSWORD \\\n--nfs-server NFS-SERVER-IP --storage-dir STORAGE-DIRECTORY --setup-nfs \nImportant: This command creates an NFS server that is not highly available. If\nyou need highly available NFS storage, you must follow your own procedures to\nconfigure the storage.  \nReplace the following values:\nReplace project with the OpenShift namespace where Cloud Pak for Data is\ndeployed. \nReplace SSH-PASSWORD with the SSH password for the NFS server. If your\npassword contains a special character, such as an exclamation mark (!),\nsurround the password with single quotation marks ('). \nReplace NFS-SERVER-IP with the IP address of the server where you want to\nconfigure NFS storage.\nReplace STORAGE-DIRECTORY with the fully qualified path of the directory\nwhere data is going to be stored on the NFS server.\n256\n-\n-\n-\n-\n-\n-\n \nBy default, the NFS server is configured with the following options:\nrw\nsync\nno_root_squash\nno_all_squash\n \nIf you want to specify different configuration options, add the --nfs-options\nOPTIONS parameter to the command. Specify multiple options as a comma-\nseparated list surrounded by parenthesis. For example: --nfs-options\n(option1,option2). For information on available configuration options,\nConfiguring the NFS Server in the Red Hat Enterprise Linux documentation. \n \n What to do next \n \nEnabling GPFS support for Db2 Warehouse databases\nConfiguring database storage\n \nParent topic:Creating an integrated database \n \n257\n1.\n2.\n3.\n4.\n1.\n Enabling GPFS support for Db2 Warehouse\ndatabases \nAs an optional storage file system, you can set up an IBM SpectrumÂ® Scale storage\ncluster to act as a persistent volume for stateful Db2Â® Warehouse databases. \n Before you begin \n \nYou need a IBM Spectrum Scale cluster that is configured to work as a cloud\nstorage repository. IBM Spectrum Scale is the new name for IBMÂ® General Parallel\nFile System. The details of setting up an IBM Spectrum Scale storage system are\nbeyond the scope of this document. For more information about the capabilities of\nIBM Spectrum Scale and the installation requirements and process, see the official\ndocumentation.You need to configure the worker nodes of your IBM Cloud Pak for\nData cluster to serve as client nodes of the IBM Spectrum Scale cluster. \nDetermine which product edition of the IBM Spectrum Scale Cluster software is\nright for your environment, based on your hardware and specific performance\ncharacteristics. There is a comprehensive listing of the available editions in the\nIBM Spectrum Scale documentation.\nDownload the IBM Spectrum Scale Cluster software from IBM Fix Central onto\nthe master node. Choose the version of the client software to match what is\ninstalled on the IBM Spectrum Scale cluster. The platform must correspond to the\narchitecture and operating system of your Cloud Pak for Data worker nodes.\nYou need to install the toolkit on all the worker nodes that use the IBM Spectrum\nScale storage. The toolkit installation instructions are available in the IBM\nSpectrum Scale documentation.\nFor a detailed set of instructions on installing IBM Spectrum Scale cluster, see\nthe Protocols Quick Overview for IBM Spectrum Scale (PDF). If you already\nhave your IBM Spectrum Scale cluster, and need to configure only the client\nnodes, review the sections of that document titled Before You Begin and Protocol\n& File System Deployment.\n \nYou need to have root access on the master nodes on the Cloud Pak for Data\ncluster and to the nodes the IBM Spectrum Scale cluster. \n About this task \n \nThis task covers installation of the IBM Storage Enabler for Containers plug-in on\nyour Cloud Pak for Data cluster, configuration of the storage class, and setting IBM\nSpectrum Scale as a persistent volume when you provision your Db2 Warehouse\ndatabase.Note: If you do not want to use the dynamically provisioned storage class\nthat is provided by the IBM Storage Enabler for Containers plug-in, use the hostPath\noption when you provision your database. Set the hostPath to point to a directory on\nthe mount point for your IBM Spectrum Scale storage cluster, and skip the\nremainder of this task. \n \n Procedure \n \nDownload the IBM Storage Enabler for Containers plug-in.The plug-in is\navailable as a free download from IBM Fix Central \n258\n2.\n3.\n4.\n5.\n6.\nAfter the installation file is downloaded, use the kubectl cp command to copy the\ninstaller to a local folder on the Cloud Pak for Data master node. Change to the\ndirectory where you put the compressed file and unpack it.tar -xzvf installer-for-ibm-\nstorage-enabler-for-containers-x.y.z-nnn.tar.gz\ncd installer-for-ibm-storage-enabler-for-containers-x.y.z-nnn/ \n \nFrom a node on the IBM Spectrum Scale storage cluster, use the mmlsconfig\ncommand to display the current configuration data for the storage cluster.The\ncommand returns information similar to the following example:Configuration data for\ncluster small.cluster:\n---------------------------------------------\nmyNodeConfigNumber 1\nclusterName small.cluster\nclusterId 6339012640885012929\nautoload yes\nprofile gpfsProtocolDefaults\ndmapiFileHandleSize 32\nminReleaseLevel 5.0.2.0\nccrEnabled yes\ncipherList AUTHONLY\nmaxblocksize 16M\n[cesNodes]\nmaxMBpS 5000\nnumaMemoryInterleave yes\nenforceFilesetQuotaOnRoot yes\nworkerThreads 512\n[common]\ntscCmdPortRange 60000-61000\nadminMode central\n \nFile systems in cluster small.cluster:\n--------------------------------------\n/dev/fs1 \n \nOn the Cloud Pak for Data master node, update the ubiquity_installer_scale.conf\nconfiguration file according to your cluster environment. Replace all the VALUE\nplaceholders in the file with the values for your cluster. You can use any text\neditor to change the file.For more information about the parameters for the\nubiquity_installer_scale.conf configuration file, see the Installation topic for IBM\nStorage Enabler for Containers. \nUse the ubiquity_installer.sh utility to apply the updated\nubiquity_installer_scale.conf settings to the relevant .yml files of the installer.\n./ubiquity_installer.sh -s update-ymls -c ubiquity_installer_scale.conf \n \nCheck the reclaimPolicy parameter is set to Retain in the yamls/storage-class-\nspectrumscale.yml file. Use a text editor to change this parameter if necessary.\nkind: StorageClass\napiVersion: storage.k8s.io/v1beta1\n259\n7.\n8.\n9.\n10.\n11.\nmetadata:\n name: \"ubiquity\"\n labels:\n   product: ibm-storage-enabler-for-containers\nreclaimPolicy: \"Retain\" \nprovisioner: \"ubiquity/flex\"\nparameters:\n backend: \"spectrum-scale\"\n filesystem: \"fs1\"\n fileset-type: \"dependent\"\n type: fileset \n \nInstall the plug-in by using the ubiquity_installer.sh command from the master\nnode../ubiquity_installer.sh -s install \nWhen the installation is complete, the command returns the following message:\n\"IBM Storage Enabler for Containers\" installation finished successfully in the Kubernetes cluster. \n \nVerify the status of the IBM Storage Enabler for Containers service. Run the\nfollowing command: ./ubiquity_cli.sh -a status \nCheck that the status of the ibm-ubiquity-db PV and PVC is Bound, and all other\nelements are available and running. \nRun a sanity test by using the ubiquity_cli.sh command:./ubiquity_cli.sh -a sanity -n\nubiquity \nWhen the command is completed successfully, it returns a message:\"IBM Storage\nEnabler for Containers\" sanity finished successfully. \n \n If you receive an error message from the server about a denied request from\ntrust.hooks.securityenforcement.admission.cloud.ibm.com, you\nmust grant the ubiquity namespace permission to download images from\ndocker.io/ibmcom. Run this permission file through the kubectl command on the\nmaster node:kubectl -n ubiquity create -f - << EOF\napiVersion: securityenforcement.admission.cloud.ibm.com/v1beta1\nkind: ClusterImagePolicy\nmetadata:\n name: ubiquity-image-policy\nspec:\n repositories:\n - name: docker.io/ibmcom/*\n   policy:\n     va:\n       enabled: false\nEOF \nThe command returns the following message when successful:\nclusterimagepolicy.securityenforcement.admission.cloud.ibm.com/ubiquity-image-policy created \nEnable the automatic quotas on the IBM Spectrum Scale cluster.mmchfs fs1 -Q yes \n \n Create a database deployment on your cluster from the Cloud Pak for Data web\nclient. Provision your Db2 Warehouse database with two or more nodes to use\n260\nthe massively parallel processing (MPP) configuration. When prompted to\nconfigure the database storage, choose the Create new storage option, and\nUse a storage template. Select the ubiquity storage class. The database uses\nthe dynamically provisioned storage that is located on the IBM Spectrum Scale\nstorage cluster. \n \n \nParent topic:Creating an integrated database \n \n261\n-\n-\n-\n Configuring database storage \nYou must use the appropriate type of storage for your integrated database. You can\nconfigure the storage before you create the database deployment, or when you\ncreate the database deployment. \n About this task \n \nDecide how you want to create the database storage:\nYou can create a persistent volume before you deploy the database, and then use\nthat as existing storage during the deployment\nAlternatively, you can create a persistent volume during deployment. You can use\na storage class to create the storage, or specify storage parameters to create the\nstorage. \n The following table shows which methods and storage types are supported for each\ntype of integrated database: \n Procedure \n \nFollow the appropriate steps for your environment:\nProvision new storage by using a storage class\nDatabase type New storage by\nusing a storage\nclass\nNew storage by\nspecifying\nstorage\nparameters\nUse existing\nstorage\nIBMÂ® Db2Â®\nAdvanced\nEnterprise\nServer Edition\nNFS hostPath (not\nsupported on\nHADR\ndeployments)N\nFS\nhostPath (not\nsupported on\nHADR\ndeployments)N\nFS\nIBM Db2 Event\nStore\nGlusterFS GlusterFSIBM\nCloud Object\nStorage bucket\nGlusterFSIBM\nCloud Object\nStorage bucket\nIBM Db2\nWarehouse\nNFS hostPathNFS hostPathNFS\nMongoDB Not supported Local persistent\nvolumeNFS\nNot supported\nPostgreSQL Local persistent\nvolumeNFS\nNot supported\nStorage type Next steps\nGlusterFS persistent volume An IBM Cloud Private cluster\nadministrator must create a\nstorage class for GlusterFS.\n Set the Reclaim Policy on the\nstorage class to Retain to\nprevent data from being deleted.\n262\n- Create new storage by specifying storage parameters\nLocal persistent volume An IBM Cloud Private cluster\nadministrator must create a\nstorage class for a local\npersistent volume using the no-\nprovisioner storage class.\nFor deployment on non-\ndedicated worker nodes, the\nIBM Cloud Private cluster\nadministrator must also label\neach node with the\ncommand:kubectl label node node-IP-\naddress icp4data=database-database-type\n For PostgreSQL, the value of\ndatabase-type is postgresql\nThe local path that is specified\nfor the database storage must\nexist on each of the worker\nnodes.\nNFS persistent volume You must have an NFS server.\nYou can optionally use the\nconfigure-database.sh script to\nset up an NFS server. For more\ninformation, see Preparing your\nIBM Cloud Private cluster for a\ndatabase.\nIn addition, an IBM Cloud\nPrivate administrator must\ncreate a storage class in IBM\nCloud Private.\nSet the Reclaim Policy on the\nstorage class to Retain to\nprevent data from being deleted.\n See the Restriction note on\nNFS storage without access to\nthe /etc/exports file.\nFor more information about\nconfiguring dynamic\nprovisioning for NFS storage,\nsee the External Storage topic\nin the kubernetes-\nincubator repository on\nGitHub.\nStorage type Next steps\nGlusterFS persistent volume An IBM Cloud Private cluster\nadministrator must create a\nstorage class for GlusterFS.\n Set the Reclaim Policy on the\nstorage class to Retain to\nprevent data from being deleted.\n263\n- Use existing storage\nhostPath An IBM Cloud Private cluster\nadministrator or team\nadministrator must mount a\nshared directory on the nodes\nwhere the database is to be\ndeployed.For dedicated\ndatabase nodes, the directory\nmust be mounted on all of the\ndedicated worker nodes.For\nnon-dedicated nodes, the\ndirectory must be mounted on\nall of the nodes in the cluster.\nFor a list of the supported file\nsystems, see the appropriate\ndocumentation:For IBM Db2\nWarehouse, see Storage\ntechnologies.For IBM Db2\nAdvanced Enterprise Server\nEdition, see Db2 environments\nwithout the Db2 pureScale\nFeature.For PostgreSQL, see\nhttps://www.postgresql.org/docs\n/current/storage.html\nIBM Cloud Object Storage\nbucket\nYou don't need to configure\nanything, but you do need your\nobject storage connection\ninformation.\nNFS persistent volume Ensure that you have an NFS\nserver with a dedicated directory\nfor storing the data. You can\noptionally use the configure-\ndatabase.sh script to set up an\nNFS server. For more\ninformation, see Preparing your\nIBM Cloud Private cluster for a\ndatabase.See the Restriction\nnote on NFS storage without\naccess to the /etc/exports file.\nStorage type Next steps\nGlusterFS persistent volume An IBM Cloud Private cluster\nadministrator or team\nadministrator must create a\npersistent volume claim.For\nmore information, see\nPersistentVolumeClaims in the\nKubernetes documentation.\n264\n-\n-\n Restriction: If you want to use NFS storage, but you do not have access to the\n/etc/exports file on the NFS server, you must mount the NFS volume on each of the\nnodes where the database is to be deployed.Then, when you deploy your database,\nselect hostPath for the storage class and point to the mount point where you\nmounted the NFS storage. \nFor dedicated database nodes, the directory must be mounted on all of the\ndedicated worker nodes.\nFor non-dedicated nodes, the directory must be mounted on all of the nodes in the\ncluster. \nhostPath An IBM Cloud Private cluster\nadministrator or team\nadministrator must mount a\nshared directory on the nodes\nwhere the database is to be\ndeployed.For dedicated\ndatabase nodes, the directory\nmust be mounted on all of the\ndedicated worker nodes.For\nnon-dedicated nodes, the\ndirectory must be mounted on\nall of the nodes in the cluster.\nFor a list of the supported file\nsystems, see the appropriate\ndocumentation:For IBM Db2\nWarehouse, see Storage\ntechnologies.For IBM Db2\nAdvanced Enterprise Server\nEdition, see Db2 environments\nwithout the Db2 pureScale\nFeature.For PostgreSQL, see\nhttps://www.postgresql.org/docs\n/current/storage.html\nFor more information about\nconfiguring a hostPath for data\nstorage, see hostPath in the\nIBM Cloud Private\ndocumentation.\nIBM Cloud Object Storage\nbucket\nYou don't need to configure\nanything, but you do need your\nobject storage connection\ninformation.\nNFS persistent volume An IBM Cloud Private cluster\nadministrator or team\nadministrator must create an\nNFS persistent volume\nclaim.See the Restriction note\non NFS storage without access\nto the /etc/exports file.\nFor more information, see\nPersistentVolumeClaims in the\nKubernetes documentation.\n265\n1.\n2.\n3.\n-\n-\n-\nTo mount the NFS volume on each of the nodes where the database is to be\ndeployed, install the NFS utilities, create a mount point and then use the mount\ncommand. For example: \nInstall the NFS utility by running the following command:yum -y install nfs-utils nfs-\nutils-lib \nCreate a path for the mount point:mkdir -p /mount-point \nReplace mount-point with the directory that you want to use. \nMount the NFS volume using the mount command. Since you don't have access\nto the /etc/exports file to pass through the mount options, you will need to add\nthem as options to the command using the -o flag.  For example:mount -t nfs -o\nrw,sync,no_root_squash,no_all_squash nfs NFS-server-address:mount-pathmount-point \nReplace the following values:\nNFS-server-address with the address of your NFS server.\nmount-path with the path of the storage on the NFS server.\nmount-point with the directory where you want to mount the storage on the\nnode.\n Refer to the official documentation for your cloud platform on the options required\nfor NFS volumes.  \n \n \n \n What to do next \n \nCreating a database deployment on the cluster \nParent topic:Creating an integrated database \n \n266\n1.\n2.\n3.\n4.\n5.\n6.\n7.\n8.\n9.\n Creating a database deployment on the cluster \nYou create a database deployment on your cluster from the IBMÂ® Cloud Pak for\nData web client. \n Before you begin \n \nEnsure that you completed the steps in Preparing your IBM Cloud Private cluster for\na database. \n About this task \n \nYou must have the Provision add-ons permission to complete this task. \nTip: If you are deploying a sandbox environment, you can use the Create with\ndefaults option to deploy the database. However, you don't have any control over\nthe resources that are allocated to the database, the storage that is used, and so\nforth. \n Procedure \n \nFrom the navigation, select Collect > My data.\nOpen the Databases tab.Restriction: This tab is displayed only if you completed\nthe previous steps under Creating an integrated database. \n \nClick Create a database.\nSelect the database type and version. Click Next.\n(IBM Db2Â® Warehouse only) Specify the number of nodes to use.You are\nconstrained by the total number of nodes in the cluster.  \n \nSpecify the number of cores to use.You are constrained by the total number of\ncores on the node. For example, if you created a node with 16 cores, you can\nspecify up to 16 cores. If the database is deployed on multiple nodes, the\nresources must exist on each node.  \n \nSpecify the amount of memory to use.You are constrained by the total amount of\nmemory on the node. For example, if you created a node with 512 GB of\nmemory, you can specify up to 512 GB. If the database is deployed on multiple\nnodes, the resources must exist on each node.  \n \nSpecify whether you want to deploy the database on dedicated nodes.It is\nrecommended that you deploy the database on dedicated nodes for production\nworkloads to ensure that the database has sufficient resources. However, this\noption is available only if you used the --add-database-node option when you\nprepared your cluster. \n(PostgreSQL only) The database can be deployed on dedicated nodes only.  \nIf you cannot select this option or if you choose not to deploy the database on\ndedicated nodes, the nodes might be used by other add-ons or services. \n \n(IBM Db2 Advanced Enterprise Server Edition only) Specify whether you want to\nEnable high availability disaster recovery (HADR).Important: HADR deployments\nrequire extra nodes for failover support. You must have nodes available with\nenough resources in your cluster before you deploy the database:\n267\n-\n-\n10.\n11.\n12.\n-\n-\n-\n-\nIf you are deploying the database on dedicated worker nodes, you must have\nat least two dedicated worker nodes plus another dedicated or non-dedicated\nworker node.\nIf you are deploying the database on non-dedicated worker nodes, you must\nhave three worker nodes available. Kubernetes searches for nodes with the\nrequested number of cores and the requested amount of memory.\n \n \nSelect which existing namespace to use for the database deployment. Click\nNext.Namespaces divide cluster resources between multiple users, applications,\nor services to ensure a fair sharing of available resources. You can use a\nnamespace to structure the resources that are assigned to the database. You\ncan also use separate namespaces for extra security. For more information, see\nNamespaces in the Kubernetes documentation. \nFor some databases, such as Db2 Warehouse and MongoDB, you can choose\nto keep your system data and user data together in a single storage location, or\nput them in separate locations. System data contains the information used by\nDb2 Warehouse to manage and configure the database. User data is the main\ndatabase data. If you decided to keep the two in separate locations, you will\nneed to specify a storage volume type, a name, and a size for both storage\nlocations. \nSpecify the storage to use for the database.The options that are available\ndepend on:\nThe type of database you are deploying\nThe storage classes or persistent volume claims that are available on the\ncluster. For example, if you do not have a persistent volume claim, you cannot\nuse existing storage.\n \nTo create new storage by using a storage class template:\nTo create new storage by specifying storage parameters:\nStorage type Details\nNFS persistent\nvolumeGlusterFS persistent\nvolume\nSelect Create new\nstorage.Select Use storage\ntemplate and specify the\nfollowing information:Select\nthe storage class to\nuse.Specify the amount of\nstorage to allocate to the\npersistent volume. You are\nconstrained by the total\namount of storage on the\nnode. For example, if you\ncreated a 1 TB storage\npartition, you can specify up to\n1000 GB.\nStorage type Details\n268\nNFS persistent volume Select Create new\nstorage.Select Define storage\nparameters and specify the\nfollowing information:If the\ndatabase supports multiple\ntypes of storage, specify NFS\nas the storage volume\ntype.The name to use for the\npersistent volume\nclaim.Specify the amount of\nstorage to allocate to the\npersistent volume. You are\nconstrained by the total\namount of storage on the\nnode. For example, if you\ncreated a 1 TB storage\npartition, you can specify up to\n1000 GB.Specify the reclaim\npolicy to use for the persistent\nvolume:Retain means that the\ndata remains on the storage\nvolume and the volume cannot\nbe used until an administrator\nreclaims it.Recycle scrubs the\ndata from the storage volume\nand makes the volume\navailable for reuse. Delete\nmeans the data and the\nstorage volume are deleted.\nSpecify the IP address of your\nNFS server.For HADR\ndeployments, specify the IP\naddress of a second NFS\nserver for failover\nsupport.Specify the directory\non the NFS server where you\nwant to store the data.\n269\nGlusterFS persistent volume Select Create new\nstorage.Select Define storage\nparameters and specify the\nfollowing information:If the\ndatabase supports multiple\ntypes of storage, specify\nGlusterFS as the storage\nvolume type.The name to use\nfor the persistent volume\nclaim.Select the storage class\nto use for the persistent\nvolume.Specify the amount of\nstorage to allocate to the\npersistent volume. You are\nconstrained by the total\namount of storage on the\nnode. For example, if you\ncreated a 1 TB storage\npartition, you can specify up to\n1000 GB.Specify the reclaim\npolicy to use for the persistent\nvolume:Retain means that the\ndata remains on the storage\nvolume and the volume cannot\nbe used until an administrator\nreclaims it.Recycle scrubs the\ndata from the storage volume\nand makes the volume\navailable for reuse. Delete\nmeans the data and the\nstorage volume are deleted.\nIBM Cloud Object Storage\nbucket\nSelect Create new\nstorage.Select Use cloud\nobject storage and specify the\nfollowing information:The\nbucket where you want to\nstore the data.The URL of your\nCloud Object Storage. Do not\ninclude the https prefix in the\nURL, for example, s3-\napi.dal-us-\ngeo.objectstorage.soft\nlayer.net.Your access key\nand secret key credentials for\nauthenticating to your cloud\nstorage.\n270\n- To use existing storage:\nhostPath Select Create new\nstorage.Select Define storage\nparameters and specify the\nfollowing information:If the\ndatabase supports multiple\ntypes of storage, specify\nhostPath as the storage\nvolume type.The name to use\nfor the persistent volume\nclaim.Specify the amount of\nstorage to allocate to the\npersistent volume. You are\nconstrained by the total\namount of storage on the\nnode. For example, if you\ncreated a 1 TB storage\npartition, you can specify up to\n1000 GB.Specify the reclaim\npolicy to use for the persistent\nvolume:Retain means that the\ndata remains on the storage\nvolume and the volume cannot\nbe used until an administrator\nreclaims it.Recycle scrubs the\ndata from the storage volume\nand makes the volume\navailable for reuse. Delete\nmeans the data and the\nstorage volume are deleted.\nSpecify the fully qualified host\npath. Remember: The path\nmust be a shared directory\nthat is mounted on the nodes\nwhere the database is going to\nbe deployed. For more\ninformation, see Configuring\ndatabase storage.\nStorage Details\nNFS persistent\nvolumeGlusterFS persistent\nvolumehostPath persistent\nvolume\nSelect Use existing\nstorage.(IBM Db2 Event Store\nonly) Ensure that Cluster\nstorage is selected.Select the\nclaim that you want to use.\n271\n13.\n14.\n15.\n16.\n17.\n18.\n-\n-\n19.\n(Db2 Event Store only) In the Local storage field, specify the XFS formatted\npartition to use for storage on the worker nodes.This storage is used for fast\nlocal persistence during computations and for persisting data in log files before it\nis written to the shared storage. \n(PostgreSQL only) In the Local path for node fields, specify either an NFS\ndirectory or a local storage directory. The directory must exist on the node.\n(PostgreSQL only) In the Backup field, specify a directory to hold the database\nbackups. This directory must be shared between the PostgreSQL worker nodes\nand have a POSIX compliant name. \nClick Next.\nOptional: Specify a new display name for the database.\nEnsure that the summary is correct and click Create.\nIf you are deploying IBM Db2 Event Store, you might have to wait several\nminutes for the database to be deployed.\nIf you are deploying IBM Db2 Warehouse, you might have to wait 2 - 40\nminutes, based on the number of worker nodes and amount of memory\nallocated to the deployment. \nThe database is ready when it shows up as Available on the Databases tab. \n \nWhen the database is ready, select Submit connection for approval from the\naction menu.Important: The connection to the database is not available in the\ncatalog until the request is approved by a user with Manage Catalog\npermissions (for example, a Data Steward). \n \n What to do next \n \nEnsure that a user with Manage Catalog permissions approves the request. The\nrequest shows on the Publish to Catalog Requests tab on their home page. After the\nrequest is approved, the database is available on the Data connections page. You\ncan use the connection when you run automated discovery to import, analyze, and\nclassify data from the database. \nIBM Cloud Object Storage\nbucket\nSelect Use existing\nstorage.Select Use cloud\nobject storage and specify the\nfollowing information:The\nbucket where you want to\nstore the data.The URL of your\nCloud Object Storage. Do not\ninclude the https prefix in the\nURL, for example, s3-\napi.dal-us-\ngeo.objectstorage.soft\nlayer.net.Your access key\nand secret key credentials for\nauthenticating to your cloud\nstorage.\nDatabase type Follow-up tasks\n272\n \nParent topic:Creating an integrated database \n \nIBM Db2 Advanced Enterprise\nServer Edition\nAs the database administrator,\nyou can:Give users access to\nthe databaseWorking with an\nintegrated IBM Db2 Advanced\nEnterprise Server Edition\ndatabase\nIBM Db2 Event Store After the database is created,\nany user can Work with the\nintegrated Db2 Event Store\ndatabase.\nIBM Db2 Warehouse As the database administrator,\nyou can:Give users access to\nthe databaseWork with the\nintegrated Db2 Warehouse\ndatabase\nMongoDB As the database administrator,\nyou can access the\nMongoDBOps Manager to\ncomplete common tasks such as\nmanaging the users that have\naccess to the database or\nconfiguring your MongoDB\nnodes. For more information,\nsee Working with an integrated\nMongoDB database.\nPostgreSQL Database administrators can\nGive users access to the\ndatabase. For more information\nabout connecting to the\ndatabase or loading data, see\nWorking with an integrated\nPostgreSQL database.\n273\n1.\n2.\n3.\n4.\n Updating the Db2 password secrets \nThe Db2 administration accounts on your IBMÂ® Cloud Pak for Data cluster are\nprotected by password secrets. During deployment of Db2 Warehouse, these\npassword secrets are automatically generated and securely stored for these\naccounts. These accounts are used by Db2 Warehouse to handle administrative\ntasks on the database. If you need to change them to comply with specific password\nregulations, or if your security situation changes, you can use this method to update\nthe password secrets at any point in time. \n About this task \n \nThese commands update the passwords for both the Db2 instance user account,\nand the Db2 admin account, which is kept in the local Db2 LDAP service. This\nchange must be run on the master node, and requires the kubectl command. You\nneed a user account with sufficient authority to run the kubectl patch and kubectl\ndelete commands in the namespace where your Db2 Warehouse instance is\nrunning. Procedure \n \nSSH into the master node (master-1) on your Cloud Pak for Data cluster.ssh account\n@MASTER-1-IP \nReplace account with the user account that has permission to run kubectl\ncommands. \nRun the following kubectl command from the command line to get the database\ninstance identifier.kubectl get pods -n NAMESPACE | grep db2wh \nReplace NAMESPACE with the namespace where your database instance is\nrunning on the cluster. The command returns a string that contains the instance\nidentifier number:  \ndb2wh-1546024110-db2u-0 \nIn this example, the instance identifier of the database is 1546024110. \nRun the following kubectl commands from your command line to update the\npassword in the secret object.kubectl patch -n NAMESPACE $(kubectl get secret -n NAMESPACE -o\nname | grep \"INSTANCE_ID-db2u-instance\") \\\n-p $\"{\\\"data\\\":{\\\"password\\\": \\\"$(echo INSTANCE_NEW_PASSWORD | base64)\\\"}}\" \nkubectl patch -n NAMESPACE $(kubectl get secret -n NAMESPACE -o name | grep \"INSTANCE_ID-db2u-ldap-\nbluadmin\") \\\n-p $\"{\\\"data\\\":{\\\"password\\\": \\\"$(echo LDAP_BLUEADMIN_PASSWORD | base64)\\\"}}\" \n Replace NAMESPACE with the namespace where your Db2 instance is running\nand INSTANCE_ID with the numerical identifier that was returned from the\nprevious step. Replace INSTANCE_NEW_PASSWORD with the new password\nfor the Db2 instance and LDAP_BLUEADMIN_PASSWORD with the new admin\npassword.  \nRestart the affected cluster pods. In the following commands, replace\nNAMESPACE with the namespace where your Db2 instance is running and\nINSTANCE_ID with the numerical identifier that was returned from the earlier\ncommand. \n Restart the Db2 LDAP pod:kubectl delete -n NAMESPACE $(kubectl get po -n NAMESPACE -o name |\ngrep \"INSTANCE_ID-db2u-ldap\") \n \nRestart the Db2 database pods:kubectl delete -n NAMESPACE $(kubectl get po -n NAMESPACE -o\n274\nname | grep -E \"INSTANCE_ID-db2u-[0-9]\")\n \n \nRestart the Db2 tools pod:kubectl delete -n NAMESPACE $(kubectl get po -n NAMESPACE -o name |\ngrep \"INSTANCE_ID-db2u-tools\") \n \nRestart the unified console:kubectl delete -n NAMESPACE $(kubectl get po -n NAMESPACE -o name |\ngrep \"INSTANCE_ID-ibm-unified-console\") \n \n \nParent topic:Creating an integrated database \n \n275\n"
      ],
      "document_passages": [
        {
          "passage_text": "You must <em>have</em> nodes available with\nenough resources in your cluster before you deploy the database:\n267\n-\n-\n10.\n11.\n12.\n-\n-\n-\n-\nIf you are deploying the database on dedicated worker nodes, you must <em>have</em>\nat least two dedicated worker nodes plus another dedicated or non-dedicated\nworker node.\nIf you are deploying the database on non-dedicated worker nodes, you must\n<em>have</em> three worker nodes available.",
          "start_offset": 22721,
          "end_offset": 23121,
          "field": "text"
        },
        {
          "passage_text": "If you are deploying IBM Db2 Event Store, you might <em>have</em> to wait several\nminutes for the database to be deployed.\nIf you are deploying IBM Db2 Warehouse, you might <em>have</em> to wait 2 - 40\nminutes, based on the number of worker nodes and amount of memory\nallocated to the deployment.",
          "start_offset": 29301,
          "end_offset": 29579,
          "field": "text"
        }
      ]
    },
    {
      "document_id": "d7252de7-4572-4990-b815-8514604b2024",
      "result_metadata": {
        "collection_id": "fbafd5e1-9cf1-c24c-0000-016eeb0a5c9f",
        "document_retrieval_source": "search",
        "confidence": 0.0504999
      },
      "enriched_text": [
        {
          "entities": [
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 40,
                    "begin": 34
                  },
                  "text": "Watson"
                },
                {
                  "location": {
                    "end": 90,
                    "begin": 84
                  },
                  "text": "Watson"
                },
                {
                  "location": {
                    "end": 229,
                    "begin": 223
                  },
                  "text": "Watson"
                },
                {
                  "location": {
                    "end": 255,
                    "begin": 249
                  },
                  "text": "Watson"
                },
                {
                  "location": {
                    "end": 289,
                    "begin": 283
                  },
                  "text": "Watson"
                },
                {
                  "location": {
                    "end": 637,
                    "begin": 631
                  },
                  "text": "Watson"
                },
                {
                  "location": {
                    "end": 12275,
                    "begin": 12269
                  },
                  "text": "Watson"
                },
                {
                  "location": {
                    "end": 13277,
                    "begin": 13271
                  },
                  "text": "Watson"
                }
              ],
              "text": "Watson",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 127,
                    "begin": 124
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 4065,
                    "begin": 4062
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 4170,
                    "begin": 4167
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 18539,
                    "begin": 18536
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 18634,
                    "begin": 18631
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 20748,
                    "begin": 20745
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 20828,
                    "begin": 20825
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 20899,
                    "begin": 20896
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 23089,
                    "begin": 23086
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 24116,
                    "begin": 24113
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 24780,
                    "begin": 24777
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 25117,
                    "begin": 25114
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 25528,
                    "begin": 25525
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 29019,
                    "begin": 29016
                  },
                  "text": "IBM"
                },
                {
                  "location": {
                    "end": 29806,
                    "begin": 29803
                  },
                  "text": "IBM"
                }
              ],
              "text": "IBM",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 1277,
                    "begin": 1267
                  },
                  "text": "Kubernetes"
                }
              ],
              "text": "Kubernetes",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 4521,
                    "begin": 4516
                  },
                  "text": "oketi"
                }
              ],
              "text": "oketi",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 5299,
                    "begin": 5290
                  },
                  "text": "secretkey"
                },
                {
                  "location": {
                    "end": 5933,
                    "begin": 5924
                  },
                  "text": "secretkey"
                }
              ],
              "text": "secretkey",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 5912,
                    "begin": 5903
                  },
                  "text": "accesskey"
                }
              ],
              "text": "accesskey",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 6017,
                    "begin": 6012
                  },
                  "text": "MinIO"
                }
              ],
              "text": "MinIO",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 7099,
                    "begin": 7093
                  },
                  "text": "1.5 GB"
                }
              ],
              "text": "1.5 GB",
              "type": "Quantity"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 7472,
                    "begin": 7466
                  },
                  "text": "0.5 GB"
                }
              ],
              "text": "0.5 GB",
              "type": "Quantity"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 8286,
                    "begin": 8280
                  },
                  "text": "100 GB"
                }
              ],
              "text": "100 GB",
              "type": "Quantity"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 9112,
                    "begin": 9104
                  },
                  "text": "RabbitMQ"
                }
              ],
              "text": "RabbitMQ",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 11152,
                    "begin": 11150
                  },
                  "text": "su"
                }
              ],
              "text": "su",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 14453,
                    "begin": 14450
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 14629,
                    "begin": 14626
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 14665,
                    "begin": 14662
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 14709,
                    "begin": 14706
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 14716,
                    "begin": 14713
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 14938,
                    "begin": 14935
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 14945,
                    "begin": 14942
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 16122,
                    "begin": 16119
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 16156,
                    "begin": 16153
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 16194,
                    "begin": 16191
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 17787,
                    "begin": 17784
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 18018,
                    "begin": 18015
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 18248,
                    "begin": 18245
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 18255,
                    "begin": 18252
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 18279,
                    "begin": 18276
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 18286,
                    "begin": 18283
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 18935,
                    "begin": 18932
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 19951,
                    "begin": 19948
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 22347,
                    "begin": 22344
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 24231,
                    "begin": 24228
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 24254,
                    "begin": 24251
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 25357,
                    "begin": 25354
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 25381,
                    "begin": 25378
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 26254,
                    "begin": 26251
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 27320,
                    "begin": 27317
                  },
                  "text": "ibm"
                },
                {
                  "location": {
                    "end": 27753,
                    "begin": 27750
                  },
                  "text": "ibm"
                }
              ],
              "text": "ibm",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 14704,
                    "begin": 14693
                  },
                  "text": "StatefulSet"
                }
              ],
              "text": "StatefulSet",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 15982,
                    "begin": 15975
                  },
                  "text": "patcher"
                }
              ],
              "text": "patcher",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 16178,
                    "begin": 16170
                  },
                  "text": "sentinel"
                }
              ],
              "text": "sentinel",
              "type": "PrintMedia"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 16352,
                    "begin": 16344
                  },
                  "text": "38.5 GBs"
                }
              ],
              "text": "38.5 GBs",
              "type": "Quantity"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 16435,
                    "begin": 16433
                  },
                  "text": "US"
                }
              ],
              "text": "US",
              "type": "Location"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 20452,
                    "begin": 20448
                  },
                  "text": "Helm"
                },
                {
                  "location": {
                    "end": 20622,
                    "begin": 20618
                  },
                  "text": "Helm"
                }
              ],
              "text": "Helm",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 20780,
                    "begin": 20773
                  },
                  "text": "Red Hat"
                },
                {
                  "location": {
                    "end": 21079,
                    "begin": 21072
                  },
                  "text": "Red Hat"
                }
              ],
              "text": "Red Hat",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 20927,
                    "begin": 20920
                  },
                  "text": "2.1.0.1"
                }
              ],
              "text": "2.1.0.1",
              "type": "IPAddress"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 24613,
                    "begin": 24603
                  },
                  "text": "Kubernetes"
                }
              ],
              "text": "Kubernetes",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 25364,
                    "begin": 25358
                  },
                  "text": "watson"
                }
              ],
              "text": "watson",
              "type": "Person"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 25596,
                    "begin": 25585
                  },
                  "text": "secrets.Two"
                }
              ],
              "text": "secrets.Two",
              "type": "Company"
            },
            {
              "model_name": "natural_language_understanding",
              "mentions": [
                {
                  "location": {
                    "end": 27429,
                    "begin": 27423
                  },
                  "text": "Tiller"
                }
              ],
              "text": "Tiller",
              "type": "Person"
            }
          ]
        }
      ],
      "metadata": {
        "parent_document_id": "d7252de7-4572-4990-b815-8514604b2024"
      },
      "extracted_metadata": {
        "sha1": "737A9807A7318372DC038E45BCAAFA4FF731A19F",
        "numPages": "13",
        "filename": "add-ons-integrations-116-144-1-13.pdf",
        "file_type": "pdf"
      },
      "text": [
        "-\n-\n-\n-\n-\n-\n-\n-\n-\n Installing the Watson Speech to Text add-on \nYou can install the Watsonâ¢ Speech to Text add-on on top of IBMÂ® Cloud Pak for\nData. \n Introduction \nThis document contains installation instructions for both Watson Speech to Text and\nWatson Text to Speech solutions. \nWatson Speech to Text (STT) provides speech recognition capabilities for your\nsolutions. The service uses machine learning to combine knowledge of grammar,\nlanguage structure, and the composition of audio and voice signals to accurately\ntranscribe the human voice. It continuously updates and refines its transcription as it\nreceives more speech. \nWatson Text to Speech (TTS) service converts written text to natural-sounding\nspeech to provide speech-synthesis capabilities for applications. It gives you the\nfreedom to customize your own preferred speech in different languages. This cURL-\nbased tutorial can help you get started quickly with the service. \n Chart details \nThis chart can be used to install a single instance of both the STT and TTS\nsolutions. The solutions can be installed separately; however, if both are installed\ntogether they share their datastores for a more efficient utilization of resources and\nsimplified support. \n Prerequisites \nCloud Pak for Data 2.1.0\nKubernetes 1.12.4 or later\nHelm 2.9.1 or later \n \nBecause the installation of the chart needs to be performed from the command line,\nyou must install and configure Helm and kubectl. \n Select the components to install \nThis chart can be used to install both the STT and TTS solutions, the following tags,\nwhich are defined in the values.yaml file, can be used to enable the installation of\neach of the different components shipped in this solution.tags:\n sttAsync: true\n sttCustomization: true\n ttsCustomization: true\n sttRuntime: true\n ttsRuntime: true \n \nsttAsync\nenables the installation of the asynchronous API to access the STT service,\nwhich corresponds to the /recognitions API endpoints. \nsttCustomization\nenables the installation of the STT customization functionality, which lets you\ncustomize the STT base models for improved accuracy and corresponds to the\n/customizations API endpoints. \nttsCustomization\nenables the installation of the TTS customization functionality, which lets you\ncustomize the TTS base voices for improved voice quality and corresponds to\nthe /customizations API endpoints. \n116\n-\n-\n-\n-\nsttRuntime\nenables the installation of the core STT functionality, which lets you convert\nspeech into text using the /recognize endpoint. Note that this component is\ninstalled if either the sttRuntime, sttCustomization, or sttAsync tags\nare set to true. \nttsRuntime\nenables the installation of the core TTS functionality, which lets you convert text\ninto speech using the /synthesize endpoint. Note that this component is\ninstalled if either the ttsRuntime or ttsCustomization tags are set to\ntrue. \nBy default all the components are enabled, but each of them can be disabled\nseparately. If you want to install STT only, you need to set ttsRuntime and\nttsCustomization to false. Similarly, if you want to install TTS only, you need\nto set sttRuntime, sttCustomization and sttAsync to false. For example,\nif you want to install STT and TTS but do not want customization capabilities, then\nyou need to set sttCustomization and ttsCustomization to false. \n Namespace \nThe first step consists of creating the Kubernetes namespace where the solution is\ninstalled. The example that follows shows how to create the namespace, which in\nthis case is named speech-services. You can use a different name for the\nnamespace. \nCopy the following YAML to a file, then run the command that follows the YAML.\napiVersion: v1\nkind: Namespace\nmetadata:\n name: speech-services \nkubectl create -f {namespace_file} \n \n MinIO object storage \nMinIO object storage is used for storing persistence data needed by speech service\ncomponents. \nPersistenceIn order to use MinIO you need to have running GlusterFS server and\ncreate storage class oketi-gluster. Information about installing GlusterFS under\nIBM Cloud Private can be found here. It is possible you have created oketi-\ngluster storage class during IBM Cloud Private installation however if not here is\nhow to do it afterwards:cat > oketi-gluster.yaml << EOF\n apiVersion: storage.k8s.io/v1\n kind: StorageClass\n metadata:\n   name: oketi-gluster\n parameters:\n   nodes: |\n     [\n       { \"Hostname\":\"HOST\", \"IP\":\"$GLUSTER_SERVER\", \"Path\":\"$GLUSTER_DIR\"},\n     ]\n   volumeType: glusterfs\n provisioner: oketi\n reclaimPolicy: Retain\n117\n volumeBindingMode: Immediate\nEOF \n \n$GLUSTER_SERVER should be the address of the GlusterFS server and $NFS_DIR\nshould be the root of the glusterfs directory. After modifying and saving the file\nas appropriately, run the command: kubectl apply -f\n       oketi-nfs.yaml \n \nIt's worth pointing out that the GlusterFS server has to be installed with sufficient\nspace size. Size calculation is described in the section Storage size calculation\nguidelines. \n \nSecretsBefore you install STT or TTS, you need to provide a secret object that is\nused by MinIO itself and by other service components that interact with MinIO. This\nsecret contains the security keys to access MinIO. \nThe secret must contain the items accesskey (5 - 20 characters) and secretkey\n(8 - 40 characters) in base64 encoding. Therefore, before creating the secret, you\nneed to perform the base64 enconding. \nThe following commands encode the accesskey and secretkey in base64.\nImportant: For security reasons, you are strongly encouraged to create an\naccesskey and a secretkey that are different from the sample keys (admin and\nadmin1234) that are shown in the examples.echo -n \"admin\" | base64\nYWRtaW4=\n \necho -n \"admin1234\" | base64\nYWRtaW4xMjM0 \n \nCreate file minio.yaml with the following secret object definition:apiVersion: v1\nkind: Secret\nmetadata:\n name: minio\ntype: Opaque\ndata:\n accesskey: YWRtaW4=\n secretkey: YWRtaW4xMjM0 \nkubectl create -f minio.yaml \n \n \nMode of operationBy default, MinIO is operating in distributed mode which\nmeans that MinIO is scheduled to run multiple instance on every worked node to\nassure storage high availability. \nIn order to optimally use HA, you have to specify number of replicas. Set the number\nof replicas for distributed mode e.g external.minio.repliacas=N where N is\nnumber of the number of cluster nodes; it must be 4 <= x <= 32 (default value is\n4 replicas). \nMinIO also operates in standalone mode which means that only one instance of\nMinIO is running on arbitrary worker node and its failure means that service will not\nbe available until new instance is running and healthy. This options is sufficient for\n118\n-\n-\n-\n-\n-\ntesting purposes but not production. \nIf you want to run MinIO in standalone mode, you can do it with setting value\nexternal.minio.mode=standalone in which case you don't have to set\nexternal.minio.replicas. \n \nStorage size calculation guidelinesObject storage is used for storing binary data\nfrom the following sources:\nBase models (for example en_US-NarrowbandModel) \nOn average, base models are each 1.5 GB. Because models are updated\nregularly, you need to multiply that amount by three to make room for at least three\ndifferent versions of each model. \nCustomization data (audio files and training snapshots) \nThe storage required for customization data depends on how many hours of audio\nyou use for training your custom models. On average, one hour of audio data\nneeds 0.5 GB of storage space. You can have multiple customizations, so you\nmust factor in additional space. \nAudio files from recognition jobs that are processed asynchronously, in case they\nneed to be queued \nThe storage required for asynchronous jobs depends on the use case. If you plan\nto submit large batches of audio files, expect the service to queue some jobs\ntemporarily. This means that some audio files are held temporarily in binary\nstorage. The amount of storage required for this purpose does not exceed the size\nof the largest batch of jobs that you plan to submit in parallel. \n \nA few examples of how to calculate storage size [GB] follow:\n6 models, 3 versions, 50 hours audio = 6 * 1.5 * 3 + 50 * 0.5 = 52\n2 models, 3 versions, 20 hours audio = 2 * 1.5 * 3 + 20 * 0.5 = 19\n \nThe default storage size, 100 GB, is a minimal starting point and is typically enough\nfor operations with two to six models and about 50 hours of audio for the purpose of\ntraining custom models. That said, it is always a good idea to be generous in\nanticipation of future storage needs. \n \n Configuration of the PostgreSQL and RabbitMQ installation \n \nCreate Local Persistent Volumes to persist dataIf either STT customization, TTS\ncustomization or STT async are included in the installation (see the previous\nsection: Select the Components to Install) an instance of the PostgreSQL database\nis installed. Additionally, if the STT async component is included in the installation,\nan instance of the RabbitMQ datastore is installed. The datastores are stateful and\nneed to leverage Kubernetes persistent volumes to persist their data. \nPostgreSQL and RabbitMQ require the availability of Kubernetes Local Persistent\nVolumes, which must be created before you install the chart. The volumes are used\nto persist the data used by the datastores so if a container restarts it can reattach to\nthe original data. Given that both PostgreSQL and RabbitMQ are configured by\ndefault for high availability (HA) with three replicas, a minimum of three volumes\nneed to be installed for each of the datastores.apiVersion: v1\nkind: PersistentVolume\n119\n-\n-\n-\n-\n-\n-\n-\n-\nmetadata:\n name: {name}\nspec:\n capacity:\n   storage: {size}Gi\n accessModes:\n - ReadWriteOnce\n persistentVolumeReclaimPolicy: Retain\n storageClassName: local-storage-local\n local:\n   path: {path}\n nodeAffinity:\n   required:\n     nodeSelectorTerms:\n     - matchExpressions:\n       - key: kubernetes.io/hostname\n         operator: In\n         values:\n         - {node} \n \nwhere:\n{name}\nis the name of the Persistent Volume (PV) to be created, and needs to be\nunique.\n{size}\ns the disk space that is allocated for the persistent volume.\n{path}\nis the path in the host machine where the persistent volume is created; for\nexample, /mnt/local-storage/storage/pv_1. Note that the permissions\nof this directory need to be open enough to allow non-root access; otherwise,\npods running as non-root are unable to mount the volume in the directory.\n{node}\nis the Kubernetesworker node where the Local Volume is to be created. You\ncan list the available nodes in your cluster by running kubectl get nodes.\n \nGiven that PostgreSQL and RabbitMQ pods are scheduled in the worker nodes\nwhere the PVs are created, the PVs need to be created in different nodes so in case\na node goes down there are still at least two healthy replicas running. Recall that a\nminimum of three replicas are needed for high availability. \n \nSetting access credentials for PostgreSQLThe PostgreSQL chart reads the\ncredentials to access the PostgreSQL database from the following secret file, which\nneeds to be created before installing the chart. You need to set the attribute\ndata.pg_su_password to the PostgreSQL password that you want (base64\nencoded). You also need to set the attribute pg_repl_password, which is the\nreplication password and is also base64 encoded, to the value you want.apiVersion: v1\ndata:\n pg_repl_password: cmVwbHVzZXI=\n120\n-\n-\n-\n-\n-\n pg_su_password: c3RvbG9u\nkind: Secret\nmetadata:\n name: user-provided-postgressql # this name can be anything you choose\ntype: Opaque \n \nTo create this secret object you need to execute the command kubectl create -f\n       {secrets_file} \n \nFinally, when installing the chart you need to set the following two values to the\nname of the secret created previously (user-provided-postgressql):\nglobal.datastores.postgressql.auth.authSecretName and\npostgressql.auth.authSecretName. \nIf you do not create the secret object, the system creates a secret object that\ncontains randomly generated passwords when the Helm chart is installed. For\nsecurity reasons you need to change the automatically generated passwords when\nthe deployment is complete. \n \n Resources required \nIn addition to the general requirements listed in Pre-installation tasks, the Watson\nSpeech to Text service has its own requirements:\nx86 is the only architecture supported at this time.\nIf you need a highly available installation a minimum of three worker nodes are\nneeded for the installation.\nThe resources required for the installation, in terms of CPUs and memory, depend\non the configuration that you select. There are two typical installation\nconfigurations: \nThe development configuration, which is the configuration that is used in the\ndefault installation, has a minimal footprint and is meant for development\npurposes and as a proof of concept. It can handle several concurrent recognition\nsessions only and it is not highly available since some of the core component\nhave no redundancy (single replica).\nThe production configuration is a highly available solution that is intended to\nrun production workloads. This configuration can be achieved by scaling up the\ndevelopment configuration after installation, see the next section.\n \nResources required by the standard Watson Speech to Text installationWhile\nthe default installation of the solution comes with the development configuration,\nyou can easily obtain the production configuration by scaling up the number of\npods/replicas of the deployment objects after installing the solution. How much to\nscale up each of the components depends on the degree of concurrency you need,\nand is limited by the amount of hardware resources available in your Kubernetes\ncluster/namespace. \nScaling up the PostgreSQL and RabbitMQ datastores \nBy default both PostgreSQL and RabbitMQ are installed with three replicas for high\navailability reasons. Each replica is typically scheduled within a different Kubernetes\nworker node if resources allow. Before performing the installation, you can configure\nthe number of replicas and the CPU and memory resources for each replica by\n121\n1.\n2.\n3.\nusing Helm values (see the Options section). \nYou can also scale up the datastores on an already running solution by changing\nthe number of replicas in the Deployment or StatefulSet objects. For example, you\ncan scale up RabbitMQ as follows:\n Edit the StatefulSet object by running kubectl edit statefulsets\n{release}-ibm-rabbitmq.\nChange the value of the spec.replicas: attribute.\nSave and close the StatefulSet object. \n \nIn the case of PostgreSQL there are two deployment objects ({release}-ibm-\npostgresql-proxy and {release}-ibm-postgresql-sentinel) and a\nStatefulSet (ibm-wc-ibm-postgresql-keeper). The deployment objects can be\nscaled up by simply running kubectl scale --replicas={n}\n{deployment_object} where {n} is the new number of replicas; for example,\nkubectl scale--replicas=3 deployment\nibm-wc-ibm-postgresql-proxy. The StatefulSet object can be scaled up\nfollowing the process described previously for PostgreSQL. \nNote that a sufficient number of Persistent Local Volumes need to be created before\nscaling up the number of replicas (in the case of the StatefulSets) so the newly\ncreated pods can mount their volumes. \nScaling up the rest of the solution \nYou can learn about the list of deployments (KubernetesDeployment objects) by\nrunning the kubectl get deployment command. You can then scale up the\nnumber of pods on each of the deployment objects to match the number of pods in\nthe production configuration, as shown in the following table. You can do this by\nusing the following command: kubectl scale --replicas={n} deployment\n{deployment_object}, where {n} is the desired number of replicas for the given\ndeployment ({deployment_object}). Table 1. Deployments\nDeployment Default number of replicas\n{release}-speech-to-text-stt-\nruntime\n1\n{release}-speech-to-text-stt-\ncustomization\n1\n{release}-speech-to-text-stt-am-\npatcher\n1\n{release}-speech-to-text-stt-\nasync\n1\n{release}-speech-to-text-gdpr-\ndata-deletion\n1\n{release}-minio 1\n{release}-rabbitmq 1\n{release}-ibm-postgressql-proxy 2\n{release}-ibm-postgressql-\nsentinel\n3\n{release}-ibm-postgressql-\nkeeper\n3\n122\n-\n-\n-\n \nTable 2. Statefulsets\n \n \nThe standard installation (development configuration) requires a total of 14.75 CPUs\nand 38.5 GBs of memory. These numbers are based on a standard installation that\nincludes the US English models only. In general, the memory requirements vary\ndepending on which models you include in the installation. \nSetting the sessions/CPU ratio \nSession in this context is one of these:\nrecognize request to the STT runtime\nsynthesize request to the TTS runtime\ntrain request to the STT Customization back-end (STT AM Patcher)train\nrequest to the STT Customization back-end (STT AM Patcher)\n \nIn order to choose resources for each of the session types, you have to do the\nfollowing calculations. \n \n \nDynamic resource calculationSTT Runtime, TTS Runtime and STT Customization\nBack-End supports automatic required memory resource calculation, which is based\non the selected number of CPUs and the selected language models. Automatic\nresource calculation is enabled by default. You can modify this behavior by setting\nfollowing values to true/false:sttRuntime.groups.sttRuntimeDefault.resources.dynamicMemory\nttsRuntime.groups.ttsRuntimeDefault.resources.dynamicMemory\nsttAMPatcher.groups.sttAMPatcher.resources.dynamicMemory \n \nWhen you set any of the previous values to false you have to specify required\nmemory yourself (see Options section below). \nDisclaimer: Disabling automatic resource calculation is not recommended and can\ncause undesired service behavior. \n \n PodSecurityPolicy Requirements \nThe predefined PodSecurityPolicy name: ibm-anyuid-psp has been verified for\nthis chart. By default the chart automatically installs the necessary RBAC roles and\nrolebindings for running the service. \nCustom PodSecurityPolicy definition:--- \n \n Pre-install steps \nRun: ./ibm_cloud_pak/pak_extensions/pre-install/clusterAdministration/labelNamespace.sh\n       ICP4D_NAMESPACE \n where ICP4D_NAMESPACE is the namespace where Cloud Pak for Data is installed\n(usually zen). \nStatefulset Number of replicas\nibm-wc-ibm-postgresql-keeper 3\nibm-wc-ibm-rabbitmq 3\n123\n-\n-\n-\nThe ICP4D_NAMESPACE namespace must have a label for the NetworkPolicy to\ncorrectly work. Only nginx and zen pods are allowed to communicate with the\npods in the namespace where this chart is installed. \n Installing the chart on IBM Cloud Pak for Data (without Red\nHat OpenShift) \nInstalling the Helm chart deploys a single IBM Watson Speech Services solution\nwith the development configuration. You can then scale up this configuration to\nsupport up to 50 concurrent recognition sessions (see the earlier sections). \nTo install the chart, run the following command:helm install --tls --name {my_release} -f\n{my_values.yaml} ibm-watson-speech-prod \nReplace {my_release} with a name for your release.\nReplace {my_values.yaml} with the path to a YAML file that specifies the\nvalues that are to be used with the install command. Specifying a YAML file is\noptional.\n \nWhen the command completes, its output shows the current status of the release. \nWhen the installation has completed and all the pods are in ready state, a series of\nHelm tests are available to validate the installation. They can be executed by using\nthe following command:helm test --tls --name {my_release} \n \nImportant:Security Notice - After completing the installation, it is strongly\nrecommended that you manually change any autogenerated passwords or\ncertificate. If not, the Helm CLI can allow a user with operator role to see the\npassword or certificate, which represents a security risk. \nPodSecurityPolicy requirementsThis chart requires a PodSecurityPolicy to be\nbound to the target namespace prior to installation. This chart has been verified with\nthe predefined ibm-restricted-psp' PodSecurityPolicy. Choose either a\npredefined PodSecurityPolicy or have your cluster administrator create a custom\nPodSecurityPolicy for you: \nCustom PodSecurityPolicy definition:--- \n \n \nUninstalling the chartTo uninstall and delete the my_release deployment, run the\nfollowing command:helm delete --tls my_release \n \nTo irrevocably uninstall and delete the my_release deployment, run the following\ncommand:helm delete --purge --tls my_release \n \nIf you omit the --purge option, Helm deletes all resources for the deployment but\nretains the record with the release name. This allows you to roll back the deletion. If\nyou include the --purge option, Helm removes all records for the deployment so\nthat the name can be used for another installation. \n \n Installing the chart on IBM Cloud Pak for Data with Red Hat\nOpenShift \nThis Helm chart deploys a single IBM Watson Speech Services instance. \nOpenShift software prerequisites\nIBM Cloud Pak for Data V2.1.0.1\n124\n-\n-\n-\n-\nKubernetes V1.11.0\nHelm V2.9.0\n Before installing the Speech Services solution, you must install and configure helm\nand kubectl. \n \nRed Hat OpenShift SecurityContextConstraints RequirementsThis chart\nrequires a SecurityContextConstraints to be bound to the target namespace prior to\ninstallation. To meet this requirement there might be cluster-scoped as well as\nnamespace-scoped pre and post actions that need to occur. \nThe predefined SecurityContextConstraints name restricted has been verified\nfor this chart. If your target namespace is bound to this SecurityContextConstraints\nresource you can proceed to install the chart. \nIf necessary, run the following command to bind the restricted\nSecurityContextConstraints to your namespace:oc adm policy add-scc-to-group restricted\nsystem:serviceaccounts:{namespace-name} \n \nThis chart also defines a custom SecurityContextConstraints that can be used to\nfinely control the permissions and capabilities needed to deploy this chart. You can\nenable this custom SecurityContextConstraints resource using the supplied\ninstructions and scripts in the pak_extension pre-install directory.\nFrom the user interface, you can copy and paste the following snippets to enable\nthe custom SecurityContextConstraints:\nCustom SecurityContextConstraints definition:fsGroup:\n type: MustRunAs\ngroups:\n- system:authenticated\nkind: SecurityContextConstraints\nmetadata:\n name: ibm-speech-scc\npriority: null\nreadOnlyRootFilesystem: false\nrequiredDropCapabilities:\n- KILL\n- MKNOD\n- SETUID\n- SETGID\nrunAsUser:\n type: MustRunAsRange\nseLinuxContext:\n type: MustRunAs\nsupplementalGroups:\n type: RunAsAny\nusers: []\nvolumes:\n- configMap\n- downwardAPI\n- emptyDir\n- persistentVolumeClaim\n125\n-\n-\n-\n1.\n-\n2.\n3.\n-\n4.\n-\n-\n5.\n-\n6.\n-\n-\n- projected\n- secret \nFrom the command line, you can run the setup scripts included under\npak_extensions. \nFor cluster admin, the pre-install instructions are located at: \npre-install/clusterAdministration/Notes.md \n For team admin the namespace scoped instructions are located at:\npre-install/namespaceAdministration/Notes.md \n \n \n \nInstalling the chartThe cluster-admin role is required to deploy IBM Watson\nSpeech Services.\nLog into OpenShift and Dockeroc login\ndocker login -u $(oc whoami) -p $(oc whoami -t) {docker-registry} \n{docker-registry} is the address of the OpenShiftDocker registry; for\nexample, docker-registry-default.apps.speech-\nopenshift.ibm.com. You can find the URL of the Docker registry in the\nOpenShift console.\nFrom the OpenShift command line tool, create the namespace in which to\ndeploy the service; for example, speech-services. Use the following\ncommand to create the namespace:oc new-project {namespace-name} \nMake sure you are pointing at the correct OpenShift project:oc project {namespace-\nname} \n{namespace-name} is the Kubernetes and Docker namespace that you\ncreated in Step 1.\nExtract the PPA archive contents:cd {compressed-file-dir}\ntar -xvfz {compressed-file-name}\ncd charts\ntar -xvfz ibm-watson-speech-prod-1.0.1.tar.gz \n{compressed-file-dir} is the directory where you downloaded\n{compressed-file-name} to.\n{compressed-file-name} is the name of the PPA file that you downloaded\nfrom IBM Passport AdvantageÂ®.\nLoad the docker images into the OpenShiftDocker registry:cd {compressed-file-\ndir}/charts/ibm-watson-speech-prod/ibm_cloud_pak/pak_extensions/pre-install/clusterAdministration\n./loadImagesOpenShift.sh --path {compressed-file-dir} --namespace {namespace-name} --registry {docker-\nregistry} \n{docker-registry} is the is the address of the OpenShiftDocker registry.\nIf successful, the docker images now exist in the OpenShiftDocker registry. \nIf you cannot access the Kubernetes command line tool, see Enabling access to\nkubectl CLI for instructions. \nCreate persistent volumes for the service.\nFor a production deployment, consider using an IBM Cloud Pak for Data\nstorage add-on or a storage option that is hosted outside the cluster.\nFor a development deployment, you can use the createLocalPVs.sh script\nthat is provided in the archive to create the local storage volumes.\n126\n7.\n8.\n9.\n10.\n-\n-\n-\n-\n-\n11.\n-\n-\nSet up required labels.A label must be added to the namespace where IBM\nCloud Pak for Data is installed (usually zen). To meet this requirement there are\ncluster-scoped pre and post actions that need to occur. Run the script that is\nprovided with the archive to add the label.cd {compressed-file-dir}/charts/ibm-watson-speech-\nprod/ibm_cloud_pak/pak_extensions/pre-install/clusterAdministration\n./labelNamespace.sh {namespace-name} \nwhere {namespace-name} is the namespace where IBM Cloud Pak for Data is\ninstalled (normally zen). \nCreate secrets.Two secret objects need to be created manually within the\n{namespace-name} namespace to set the access credentials for the MinIO\nand PostgreSQL datastores. In order to set credentials for MinIO see the\nSecretssubsection within the Configure MinIO object storage section. In order to\nset credentials for PostgreSQL and RabbitMQ see the subsection Setting\naccess credentials for PostgreSQL within the Installation appendix. \nFetch the Docker registry secret that is to be used to pull images within the\ncluster (imagePullSecret):oc get secrets | grep default-dockercfg \nEdit values in the values.yaml file, which is stored in the {compressed-\nfile-dir}/charts/ibm-watson-speech-prod directory.\nAt a minimum, you must provide your own values for the following configurable\nsettings:\nSet global.icpDockerRepo to the Docker registry URL, including the\nnamespace. For example, docker-\nregistry.default.svc:5000/{namespace-name}.\nSet global.imagePullSecretName to the name of the Docker registry\nsecret obtained in the previous step.\nSet global.image.repository to the same value you set\nglobal.icpDockerRepo.\nSet global.image.pullSecret to the same value you set\nglobal.imagePullSecretName.\n See the section Select the components to install for information on how to\nselect the components to install. Additionally, read the Installation appendix and\nConfiguration section to learn more about the installation configuration.\nAfter you define any custom configuration settings, you can install the chart from\nthe Helm command line interface. Enter the following command from the\ndirectory where the package was loaded in your local system:helm install --namespace\n{namespace-name} --name {release-name} {compressed-file-dir}/charts/ibm-watson-speech-prod/ --tiller-\nnamespace {tiller-namespace} \n{tiller-namespace} is the namespace where Tiller is installed within the\ncluster (typically the zen namespace).\n{release-name} is the name of the Helm release.\n \n \n Configuration \nThe Helm chart has the following values that you can override by using the --set\nparameter with the install command. For example:helm install --tls --set\nimage.repository={my_image} stable/ibm-datapower-dev \n \nAlternatively, you can provide a YAML file that specifies the values with the\n127\ninstall command. For example:helm install --tls -f {my_values.yaml} \n \nLanguage model selectionYou can perform an installation that includes only a\nsubset of the language models/voices in the catalog. Installing all of the\nmodels/voices in the catalog substantially increases the memory requirements.\nTherefore, it is strongly recommended that you install only those languages that you\nwill use. \nYou can select the languages to be installed by checking or unchecking each of the\nmodels/voices in global.sttModels.* or global.ttsVoices.* values. By\ndefault, the dynamic resource calculation feature is enabled; it automatically\ncomputes the exact amount of memory that is required for the selected\nmodels/voices. \nIt is also possible to install ad hoc models/voices that was not released with this\nversion. You need to download special package containing data for the\nmodels/voices, upload it into the cluster the same way as main package and specify\nfollowing options during installation. \nTable 3. Language model selection\n \nAs an example, assume that there is a new broadband model for Czech language\nthat was released as an ad hoc model for current Speech on IBM Cloud Private\nrelease. In order to enable it during update, specify following options during\ninstallation. For this example, $modelName is csCSBroadbandModel and\n$catalogName is cs-CS_BroadBandModel:helm upgrade RELEASE CHART --set\nglobal.sttModels.csCSBroadbandModel.catalogName=cs-CS_BroadBandModel --set\nglobal.sttModels.csCSBroadbandModel.size=500 [OTHER-FLAGS]\n \n \n \nStorage of customer data (STT Runtime and AMC Patcher)By default, payload\ndata, including audio files, recognition hypotheses, and annotations, are temporarily\nstored in the running container. You can disable this behavior by checking STT\nRuntime | Disable storage\nof customer data option. Checking this option also removes sensitive\ninformation from container logs. \n \nOptionsThe following options apply to an IBM Watson Speech Services runtime\nconfiguration. \nValue Description\nglobal.sttModels.$modelN\name.catalogName\nModel name as it is found in\ncatalog.\nglobal.sttModels.$modelN\name.size\nMemory footprint used to\ncalculate memory requirements.\nglobal.ttsVoices.$voiceN\name.catalogName\nVoice name as it is found in\ncatalog.\nglobal.ttsVoices.$voiceN\name.size\nMemory footprint used to\ncalculate memory requirements.\n128\n"
      ],
      "document_passages": [
        {
          "passage_text": "This chart <em>has</em> been verified with\nthe predefined ibm-restricted-psp' PodSecurityPolicy. Choose either a\npredefined PodSecurityPolicy or <em>have</em> your cluster administrator create a custom\nPodSecurityPolicy for you: \nCustom PodSecurityPolicy definition",
          "start_offset": 19899,
          "end_offset": 20146,
          "field": "text"
        },
        {
          "passage_text": "There are two typical installation\nconfigurations: \nThe development configuration, which is the configuration that is used in the\ndefault installation, <em>has</em> a minimal footprint and is meant for development\npurposes and as a proof of concept. It can handle several concurrent recognition\nsessions only and it is not highly available since some of the core component\n<em>have</em> no redundancy (single replica).",
          "start_offset": 12606,
          "end_offset": 13006,
          "field": "text"
        }
      ]
    }
  ]
}
